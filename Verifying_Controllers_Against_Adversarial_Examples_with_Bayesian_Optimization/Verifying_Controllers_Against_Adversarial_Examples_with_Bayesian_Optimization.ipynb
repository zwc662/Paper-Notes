{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://arxiv.org/abs/1802.08678\">\n",
    "Verifying Controllers Against Adversarial Examples with Bayesian Optimization</a></h1>\n",
    "Shromona Ghosh et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary</h2>\n",
    "\n",
    "* Use boolean combinations of smooth functions on the trajectories as safety specification\n",
    "\n",
    "* Use Baysesian Optimization to actively test the controller to find adversarial counterexamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Motivation</h2>\n",
    "\n",
    "* Conventional techniques in designing robust controllers rely on simple linear models of the underlying system.\n",
    "    * Either overly conservative\n",
    "    \n",
    "    * Or inaccurate due to overapproximation, e.g. cannot capture nonlinear effects\n",
    "    \n",
    "* Reinforcement learning can generate high fidelity controllers\n",
    "    * No formal guarantees for safety\n",
    "\n",
    "* Formal safety certificates by using formal methods\n",
    "    * Curse of dimensionality\n",
    "    * Only use simple system dynamics\n",
    "    \n",
    "* Falsification tests the system in various environments seeking for adversarial examples\n",
    "    * Perturbations must be meaningful for dynamic systems\n",
    "* Test black-box systems by using smart search techinques\n",
    "    * Sequential search algorithms based on heuristics, e.g. CMA-ES, Simulated Annearling, does not utilize information of previous simulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Formulation</h2>\n",
    "\n",
    "* A closed-loop system uses a model parameterizing environment uncertainty with $w\\in\\mathbb{W}$\n",
    "\n",
    "* If the system remain safe under all uncertain scenarios, then it satisfiies the safety specfication $\\forall w\\in\\mathbb{W}, \\varphi(w)>0$\n",
    "\n",
    "* Test wether there is an adversarial example $w\\in\\mathbb{W}$ s.t. $\\varphi(w)<0$ and minimize the test cost, e.g. number of simulations\n",
    "$$argmin_{w\\in\\mathbb{W}} \\varphi(w)$$\n",
    "\n",
    "* Key problem is that functional dependence $\\varphi(w)$ and $w$ is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "* Safety Specification\n",
    "$$\\varphi:= \\mu|\\neg\\mu|\\varphi_1\\vee\\varphi_2|\\varphi_1\\wedge\\varphi_2$$\n",
    "where $\\mu:\\Xi\\rightarrow\\mathbb{R}$ returns the 'robustness' of a trajectory $\\xi\\in\\Xi$\n",
    "\n",
    "$$\\mu(\\xi):=\\mu(\\xi), (\\varphi_1\\vee\\varphi_2)(\\xi):=min(\\varphi_1, \\varphi_2)),\\\\\n",
    "  \\neg\\mu(\\xi):=-\\mu(\\xi), (\\varphi_1\\wedge\\varphi_2):=max(\\varphi_1, \\varphi_2)),$$\n",
    "  \n",
    "* Gaussian Process\n",
    "    * $\\hat{\\\\\\mu}(w)$ is the observed target function $\\mu(w)$ with Gaussian noise $\\hat{\\\\\\mu}(w)=\\mu(w)+\\omega$ where $\\omega\\sim\\mathcal{N}(0,\\sigma^2)$.\n",
    "    * Given $n$ observation $y_n=(\\hat{\\\\\\mu}(w_1), \\hat{\\\\\\mu}(w_2),\\ldots, \\hat{\\\\\\mu}(w_n))$ and their corresponding input $W_n=\\{w_1, w_2,\\ldots, w_n\\}$, the posterior over function $\\mu(w)$ has expectation $m_n(w)$, covariance $k_n(w,w) and variance $\\sigma_n(w)$ where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "m_n(w)&=&k_n(w)(K_n+I_n\\sigma^2)^{-1}y_n\\\\\n",
    "k_n(w,w')&=&k(w,w')-k_n(w)(K_n+I_n\\sigma^2)^{-1}k^T_n(w')\\\\\n",
    "\\sigma^2_n(w)&=&k_n(w,w')\\\\\n",
    "\\\\\n",
    "k_n(w)&=&[k(w,w_1),\\ldots, k(w,w_n)]\\\\\n",
    "[K_n](i,j)&=&k(w_i,w_j)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "* Bayesian Optimization\n",
    "    * The optimization function based on GP is $w_n=argmin_{w\\in\\mathbb{W}} m_{n-1}(w)-\\beta^{1/2}_n\\sigma_{n-1}(w)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Approach</h2>\n",
    "\n",
    "\n",
    "* `Parse Tree` $\\mathcal{T}$: given a specification formula $\\varphi$, the corresponding parse tree $\\mathcal{T}$ has leaf nodes that corresponding to function predicates(atomics), while other nodes are max(disjunctions) and min(conjunctions) (and negation????). Use the quantified specification to represent the tree\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\varphi(w)&=&(\\mu_1\\wedge\\mu_2)\\vee\\ldots\\\\\n",
    "&\\Rightarrow& max(min\\ldots)\\\\\n",
    "\\mathcal{T}(\\mu_1(w),\\ldots, \\mu_q(w))&=&\\varphi(w) \n",
    "\\end{eqnarray}\n",
    "\n",
    "* The distribution of the tree is bounded with high probability by the lower-confidence interval of one of the predicates. Consider the **lower bounds** within the confidence intervals of each $\\mu_i$ in the tree \n",
    "$$l_1=m^i_{n-1}(w)-\\beta_n^{1/2}\\sigma^i_{n-1}(w)$$\n",
    "\n",
    "* Then a heuristic solution to BO optimization is to find a $w$ that maximally approaching the lower bounds of the confidence intervals of all the predicates. \n",
    "$$w=argmin_{w\\in\\mathbb{W}} \\varphi(l_1(w),l_2(w),\\ldots,l_q(w))$$\n",
    "After $w$ is solved, add $w$ back to $W$ and start next propagation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Algorithm</h2>\n",
    "\n",
    "The following example gives an example on how to solve the optimization problem $w=argmin_{w\\in\\mathbb{W}} \\varphi(l_1(w), l_2(w), \\ldots,l_q(w))$\n",
    "\n",
    "* The experiment uses cartpole environment from openAI gym. The observations are continuous values while the action space is discretized. \n",
    "\n",
    "* The robustness of a trajectory is evaluated depending on the following 3 aspects:\n",
    "    * Always stay within the region (-2.4, 2.4)\n",
    "    * Maintain a momentum >=-2.0 and <= 2.0\n",
    "    * The angle made by the cartpole should <=0.2 within the rest position\n",
    "\n",
    "* Safety specification is that **the cartpole should always satisfy at least one of the listed conditions**. Therefore, there are 3 sub predicates evaluating the robustness of each the 3 conditions and the final predicate is the maximum of the 3 robustness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In sub-predicate 1, evaluates the robustness of staying within range (-2.4, 2.4)\n",
    "def pred1(traj):\n",
    "    traj = traj[0]\n",
    "    x_s = np.array(traj).T[0]\n",
    "    return min(2.4 - np.abs(x_s))\n",
    "\n",
    "### In sub-predicate 2, evaluate the robustness of maining momentum out of range (-2.0, 2.0)\n",
    "def pred2(traj):\n",
    "    traj_ = traj[0]\n",
    "    mass = traj[1]['mass']\n",
    "    v_s = np.array(traj_).T[1]\n",
    "    return min(2. - np.abs(mass*v_s))\n",
    "\n",
    "### In sub-predicate 3, evaluate the robustness of keeping pole angle within the range of (-0.2, 0.2) from vertical position\n",
    "def pred3(traj):\n",
    "    traj=traj[0]\n",
    "    theta=np.array(traj).T[2]\n",
    "    return min(0.2 - np.abs(theta))\n",
    "\n",
    "### In final predicate, evaluate the maximum of the robustness values of the 3 sub-predicates\n",
    "pred = lambda traj : np.amax([pred1(traj), pred2(traj), pred3(traj)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a policy, the algorithm randomly choose different sets of uncertainty parameters `X_ns` within the pre-defined ranges. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(-0.05, 0.05)] * 4 # Bounds on the state\n",
    "bounds.append((0.05, 0.15)) # Bounds on the mass of the pole\n",
    "bounds.append((0.4, 0.6)) # Bounds on the length of the pole\n",
    "bounds.append((8.00, 12.00)) # Bounds on the force magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a controller, the agent can generate trajectories with the sets of uncertainty parameters. \n",
    "\n",
    "* The robustness values of each sub predicate including the final predicate in all trajectories (and corresponding uncertain parameters) are estimated and collected in `Y`.\n",
    "\n",
    "* Then the algorithm builds a Gaussian Process regression model based on the uncertainty parameters `X_ns` and their corresponding robustness values `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "self.ns_GP = GPy.models.GPRegression(X_ns, Y,\n",
    "                                        kernel=copy.deepcopy(self.kernel),\n",
    "                                        normalizer=self.normalizer)\n",
    "self.ns_GP.optimize_restarts(self.optimize_restarts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then Bayesian Optimization iterations begin. \n",
    "    * In each iteration, the GP regression model is used to evaluate the mean `m` and variance `v` of the robustness value from the curret `X_ns`. \n",
    "    * The algorithm builds a function that uses GP regression model to calculate the lower bound of the confidence interval\n",
    "$$l_1=m^i_{n-1}(w)-\\beta_n^{1/2}\\sigma^i_{n-1}(w)$$\n",
    "\n",
    "* There are multiple optimization models to choose from. \n",
    "    * If the optimization method is `LBFGS`, then the gradients of the mean and variance are calculated. \n",
    "    * If the optimization method is `sample optimization`, then the gradients are not calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bulid a function estimating the mean m, variance v and lower bound of the confidence interval m - self.k*np.sqrt(v)\n",
    "def f(X_ns):\n",
    "    ## Predict the mean and variance\n",
    "    m,v = self.ns_GP.predict(X_ns)\n",
    "    \n",
    "    ## If using lbfgs, then predict the gradients of the mean, variance and f\n",
    "    if isinstance(self.optimizer, lbfgs_opt):\n",
    "        dm,dv = self.ns_GP.predictive_gradients(X_ns)\n",
    "        dm = dm[:,:,0]\n",
    "        df = dm - (self.k/2)*(dv/np.sqrt(v))\n",
    "        \n",
    "return m - self.k*np.sqrt(v), df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The optimizer randomly chooses initial state and returns the estimated the uncertainty parameters that minimize the robustness value predicted by the Gaussian Process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The returned f is the robustness value at optimal solution x\n",
    "x,f = self.optimizer.optimize(f=lambda x: f(x)[0], df = lambda x:f(x)[1])\n",
    "## For lbfgs, the optimal solution can be solved by using the gradient `df`.\n",
    "## For sample optimization, the optimal solution is sampled.\n",
    "## The code provieds some other algorithms besides those two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given uncertainty parameters `x`, the agent generates trajectories correspondingly. The corresponding robustness values are evaluated and `X_ns` is updated. Gaussian Process regression model and $f$ are also updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ns_GP.set_XY(X_ns,np.vstack((self.ns_GP.Y, np.atleast_2d(f_x))))\n",
    "self.ns_GP.optimize_restarts(self.optimize_restarts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
