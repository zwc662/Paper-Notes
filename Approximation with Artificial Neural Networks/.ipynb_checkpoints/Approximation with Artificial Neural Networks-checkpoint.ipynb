{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf\">\n",
    "Approximation with Artificial Neural Networks</a></h1>\n",
    "Balázs Csanád Csáji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "========\n",
    "\n",
    "* Prove the **universal approximation theorem** which claims that \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation Power\n",
    "=============\n",
    "\n",
    "A single layer perceptron consists of a single neuron with adjustable synaptic weights and bias. \n",
    "\n",
    "<h3>Computation of Boolean Function</h3>\n",
    "\n",
    "By using threshold function as an activation function, then every boolean functions can be computed with a multilayer. perceptron with one hidden layer.\n",
    "\n",
    "><font color='green'>\n",
    "   Function $f:\\{0,1\\}^n\\rightarrow\\{0,1\\}$ can be written in BNF.\n",
    "   $$f(x_1, x_2,\\ldots, x_n\\}=c_1\\vee c_2\\vee\\ldots\\vee c_p$$\n",
    "   $$where\\ c_j = x^{j}_{1}\\wedge x^{j}_{2}\\wedge\\ldots\\wedge x^{j}_{r}\\wedge\\neg x^{j}_{r+1}\\wedge \\neg x^{j}_{r+2}\\wedge\\ldots\\wedge x^{j}_{r+s}$$\n",
    "   Use one neuron for each clause $c_j$, where $w^{j}_i=1$ if $i\\in[1,r]$, $w^{j}_i=-1$ if $i\\in[r+1, r+s]$ and $w^{j}_i=0$ if $r+s+1\\leq i\\leq n$ with $b=0.5-r$:\n",
    "   $$0.5-r+\\sum^n_{i=1} w^{j}_ix^{j}_i=0.5- (r - \\sum^r_{i=1} x^{j}_i) - \\sum^{r+s}_{i=r+1}x^{j}_i\\geq 0$$\n",
    "   For the output layer, all the outputs of the neurons are added up with all weights equal to $1$ and the threshold equal to $-0.5$.<br>\n",
    "   For a clause $c_j$ if there are $m$ numbers of falsifying assignment, e.g. among which $k$ numbers of $x_i^{j}=0$ for $i\\in[1,r]$ and $m-k$ numbers of $x_i^{j}=1$ for $i\\in[r+1,r+s]$, the output of the neuron of $c_{j}$ is $0.5-m$.\n",
    "    </font>\n",
    "   \n",
    "   Formalise the output of a MLP with m inputs, one hidden layser, n hidden units and activation function $\\varphi$ as \n",
    "   $$(A_nf)(x_1, \\ldots, x_m)=\\sum^n_{j=1}w_j\\varphi(\\sum^m_{i=1} a_{ij}x_i + b_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Universal Approximation Theorem</h3>\n",
    "\n",
    "It is the multilayer feedford architecture, not activation function, that enables the universal approximation.\n",
    "\n",
    "**Universal approximation theorem**\n",
    "\n",
    "* Let $\\varphi()$ be an arbitrary activation function. Let $X\\subset R^m$ and $X$ is compact. The space of continuous functions on $X$ is denoted by $C(X)$. Then $\\forall f\\in C(X), \\forall \\epsilon>0:\\exists n\\in N, a_{ij},b_i, w_i\\in R, i\\in\\{1\\ldots n\\}, j\\in\\{1\\ldots m\\}$:\n",
    "$$(A_nf)(x_1, \\ldots, x_m)=\\sum^n_{j=1}w_j\\varphi(\\sum^m_{i=1} a_{ij}x_i + b_j)$$\n",
    "as an approximation of the function $f(\\cdot)$; that is \n",
    "$$||f-A_n f||\\leq \\epsilon$$\n",
    "Or measure $L^\\infty$ of the distance across all the inputs\n",
    "$$||f-A_n f||_\\infty=sup_{\\underline{x}\\in X}|f(\\underline{x})-(A_n f)(\\underline{x})|$$\n",
    "Or measure the average performance across the inputs\n",
    "$$||f-A_n f||_p = \\sqrt[p]{\\int_X |f(x)-(A_n f)(\\underline{x})|^p d\\mu(\\underline{x})}$$\n",
    "\n",
    "\n",
    "<h3>Build mother function</h3>\n",
    "\n",
    "Build a mother function for the approximation from the difference of two activation functions with shifted input and bias.\n",
    "$$\\Psi(x,a,b)=\\varphi(ax+b)-\\varphi(ax-b)$$\n",
    "\\begin{align}\n",
    "\\Psi_i(x)&=\\Psi(x-x_i, a_i, b_i) \\\\\n",
    "&=\\varphi(a(x-x_i) +b) - \\varphi(a(x-x_i) -b)\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "(A_{2n}f)(x) &=\\sum^n_{i=1}w_i \\Psi_i(x)\\\\\n",
    "&=\\sum^n_{i=1}w_i\\varphi_i(a(x-x_i)+b)-\\sum^n_{i=1}w_i\\varphi_i(a(x-x_i)-b))\n",
    "\\end{align}\n",
    "\n",
    "The built function should have some useful property such as\n",
    "* has zero or small value outside a range\n",
    "* forms a Haar function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special Cases\n",
    "=============\n",
    "\n",
    "Prove the universal approximation theorem for the follwing 3 popular activation functions\n",
    "* threshold function\n",
    "* piecewise linear function\n",
    "* logistic function\n",
    "\n",
    "<h3>B-Splines</h3>\n",
    "\n",
    "B-splines are piecewise functions that can be translated and dilated from the mother functions \n",
    "\\begin{align}\n",
    "B_1(x) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "B_2 &=  B_1 * B_1=\\int^\\infty_\\infty B_1(y)B_1(x-y)dy=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1+x & \\quad -1\\leq x \\leq 0 \\\\\n",
    "            1-x & \\quad 0<x\\leq 1\\\\\n",
    "            0 & \\quad elsewhere\n",
    "        \\end{array}\n",
    "    \\right.\\\\   \n",
    "B_3 &=  B_1 * B_1 * B1\\\\\n",
    "\\ldots\n",
    "\\end{align}\n",
    "\n",
    "* $B_1$ is piecewise constant. \n",
    "* $B_2$ is piecewise linear \n",
    "* $B_3$ is piecewise quadratic, $\\ldots$.\n",
    "\n",
    "**Theory of splines**\n",
    "* Every continuous function in $R^n$ (by generalising B-spline to $R^n$ space) can be well approximated with the linear combination of this kind of functions.\n",
    "\n",
    "\n",
    "<h3>Threshold function as activation function</h3>\n",
    "\n",
    "$\n",
    "\\varphi(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\geq 0 \\\\\n",
    "            0 & \\quad x < 0\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$\n",
    "\n",
    "\n",
    "**Approximation with the threshold function**\n",
    "\n",
    "* Any arbitrary continuous function, defined on $[0,1]$ can be arbitrary well uniformly approximated by a multilayer feed-forward neural network with one hidden layer (that contains only finite number of neurons) using the threshold activation functions in the hidden layer and a linear neuron in the output layer.\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the ***threshold function***. Then $\\forall f\\in C([0,1]), \\forall \\epsilon>0:\\exists n\\in N, w_i, b_i\\in R, i\\in\\{0,\\ldots, n\\}:$\n",
    "$$(A_n f)(x)=\\sum^n_{i=1}w_i\\varphi(x+b_i)$$ \n",
    "as an approximation of function $f(\\cdot)$\n",
    "$$sup_{x\\in[0,1]}|(A_n f)(x)-f(x)|<\\epsilon$$\n",
    "\n",
    "><font color='green'>\n",
    "**Proof**<br>\n",
    "    Build the 1st order B-spline mother function (Haar function):\n",
    "    \\begin{equation}\n",
    "    \\Psi(x, 1, \\frac{1}{2})=\\varphi(x+\\frac{1}{2})-\\varphi(x-\\frac{1}{2})== \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    \\end{equation} \n",
    "    Obviously $\\Psi(x, 1, \\frac{1}{2})=B_1(x)$.                                             \n",
    "    Following B-spline theory, every continous function can be approximated by the translation and dilation of this function.\n",
    "    </font>\n",
    "\n",
    "If $n+1$ Haar functions are wanted, the simplest construction is\n",
    "$$(A_{n+1}f)(x)=\\sum^n_{i=0} w_i\\Psi_i(x, b), \\forall i\\in\\{0,\\ldots, n\\}:w_i=f(x_i), b= \\frac{1}{2n}$$\n",
    "Assume $x_i=\\frac{i}{n}, b=\\frac{1}{2n},\\Psi_i(x,b)=\\Psi(x-x_i, b), \\varphi_i(x)=\\varphi(x-x_i),\\forall i\\in\\{0\\ldots n\\}$, then **in the original paper, the equation $w_i\\Psi_i(x, b) = \\sum^n_{i=0} w_i(\\varphi_{i}(x-b)-\\varphi_{i}(x+b))$ conflicts with definition.** The correct transmission of $\\Psi_i(x, b)$ is as follows.\n",
    "><font color='green'>\n",
    "\\begin{align}\n",
    "\\Psi_i(x-b) &= \\Psi(x-x_i, b)\\\\\n",
    "&= \\varphi(x- x_i + b)-\\varphi(x-x_i-b)\\\\\n",
    "&= \\varphi(x - \\frac{i}{n} + \\frac{1}{2n}) - \\varphi(x - \\frac{i}{n} - \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - \\frac{i}{n}) + \\frac{1}{2n}) - \\varphi((x - \\frac{i + 1}{n}) + \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - x_{i}) + b) - \\varphi((x - x_{i+1}) + b)\\\\\n",
    "&= \\varphi_{i}(x+b)-\\varphi_{i+1}(x+b)\n",
    "\\end{align}\n",
    "Note that since $x\\in[0,1]$, $\\varphi_{n+1}(x+b)=\\varphi(x - 1 - \\frac{1}{2n})=0$. Then\n",
    "\\begin{align}\n",
    "\\sum^n_{i=0} w_i\\Psi_i(x, b) &= \\sum^n_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i}(x-b)) \\\\\n",
    "&= \\sum^{n}_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i+1}(x+b))\\\\\n",
    "&= w_0\\varphi_0(x+b) + \\sum^n_{i=1} (w_i-w_{i-1})\\varphi_i(x+b)\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "Therefore, $n+1$ neurons rather then $2(n+1)$ are needed. The weights $\\{w_i\\}_n$ for the compressed neurons can be represented as\n",
    "$$\n",
    "w_i' = \\left\\{\n",
    "        \\begin{array}\n",
    "            f(x_0) & \\quad i=0 \\\\\n",
    "            f(x_i) - \\sum^{i-1}_{j=0}w_j' & \\quad i \\in(0, n]\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$$\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    When $i=0$, $w_0'=w_0=f(x_0)$\n",
    "    <br>\n",
    "    When $i>0$, $w_i'=w_i-w_{i-1}=f(x_i)-f(x_{i-1})$. Then\n",
    "    \\begin{align}\n",
    "    \\sum^{i-1}_{j=0}w_j' &= w_0' + \\sum^{i-1}_{j=1}(w_j-w_{j-1}) \\\\\n",
    "    &= f(x_0) + \\sum^{i-1}_{j=1}(f(x_{j})-f(x_{j-1}))\\\\\n",
    "    &= f(x_{i-1})\\\\\n",
    "    &= f(x_i)-w_i'\n",
    "    \\end{align}\n",
    "    Therefore, $w_i'=f(x_i)-\\sum^{i-1}_{j=0}w_j'$. \n",
    " </font>\n",
    "\n",
    "Optimal approximation with biases not depending on $f$\n",
    "\\begin{align}\n",
    "x &= \\frac{i-1}{n}\\\\\n",
    "w_i^{opt} &= \\frac{\\mathcal{X}(f,i)}{2}-\\sum_{j=1}^{i-1}w_j\\\\\n",
    "\\mathcal{X}(f,i)&= sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x) \\\\\n",
    "(A_n^{opt} f)(x)&=\\sum^n_{i=1} w_i^{opt}\\varphi(x-x_i)\n",
    "\\end{align}\n",
    "\n",
    "To estimate the error of the approximation, define the modulus of continuity:\n",
    "$$\\omega(f;\\delta)=sup\\{|f(x)-f(y)|:x,y\\in D_f\\wedge |x-y|\\leq \\delta\\}$$\n",
    "\n",
    "* If $f$ is continuous and $lim_{n\\rightarrow\\infty}a_n=0$, then $lim_{n\\rightarrow\\infty}\\omega(f;a_n)=0$\n",
    "* $\\forall \\epsilon >0:\\omega(id;\\epsilon)=\\epsilon$ where $id(x)=x$ is identity function\n",
    "\n",
    "**Theorem**: $\\forall n\\in N:\\forall f\\in[0,1]\\rightarrow R:||f-A_n^{opt}f||_\\infty\\leq\\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "For an $x\\in[x_i, x_{i+1}]$,\n",
    "\\begin{array}\n",
    "(A_n^{opt}f(x) &= \\sum^n_{j=1}w_j^{opt}\\varphi(x-x_j) \\\\\n",
    "&=  \\sum^i_{j=1}w_j^{opt}\\\\\n",
    "&= w_i^{opt} +  \\sum^{i-1}_{j=1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}-\\sum^{i-1}_{j=1}w_j^{opt} + \\sum_{j=1}^{i-1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}\n",
    "\\end{array}\n",
    "\n",
    "Then formulate the error of the approximation in $[x_i, x_{i+1})$ by\n",
    "\n",
    "$$\n",
    "d_i(f,A^{opt}_n f) = sup_{x\\in[x_i, x_{i+1})} |f(x)-(A_n^{opt}f)(x)|\n",
    "=sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{\\mathcal{X}(f, i)}{2}| \n",
    "$$\n",
    "\n",
    "Because\n",
    "\n",
    "\\begin{array}\n",
    "\\\\\\omega(f;\\frac{1}{n})&=sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\}\\\\\n",
    "&\\geq sup\\{|f(x)-f(y)|:x,y\\in[x_i, x_{i+1})\\}\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} f(x) - inf_{y\\in[x_i, x_{i+1})}f(y)\\\\\n",
    "&= 2\\cdot (sup_{x\\in[x_i, x_{i+1})}f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2})\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2}|\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x)-\\frac{\\mathcal{X}(f, i)}{2}|\\\\\n",
    "&= 2\\cdot d_i(f,A^{opt}_n f)\n",
    "\\end{array}\n",
    "\n",
    "Since $\\omega(f;\\frac{1}{n})$ does not depend on $i$, $||f-A^{opt}_n f||_\\infty=max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)\\leq \\frac{\\omega(f;\\frac{1}{n})}{2}$.\n",
    "\n",
    "**Corollary for convergence**: If $f$ is continuous then $lim_{n\\rightarrow \\infty}||f-A_n^{opt} f||_\\infty = 0$.\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    \\begin{align}\n",
    "    lim_{n\\rightarrow\\infty}||f-A_n^{opt}f||_\\infty &=  lim_{n\\rightarrow\\infty}|| max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)||_\\infty \\\\\n",
    "    & \\leq  lim_{n\\rightarrow\\infty}\\frac{\\omega(f;\\frac{1}{n})}{2} \\\\\n",
    "    & = lim_{n\\rightarrow\\infty} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\} \\\\\n",
    "    & = lim_{\\delta\\rightarrow 0} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\delta\\}\\\\\n",
    "    & = 0\\ \\text{ due to continuity of $f(x)$}\n",
    "    \\end{align}\n",
    "    \n",
    " </font>\n",
    "\n",
    "**Theorem for optimality**: $A^{opt}_n f$ is optimal in the sense that if the biases are independent from $f$, then if \n",
    "$(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})\\vee(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$\n",
    "then $\\exists f\\in[0,1]\\rightarrow R:||f-A_nf||_\\infty > \\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "Firstly consider the case $(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})$. If $\\forall j\\in\\{0,\\ldots, n\\}: x_{j+1}-x_j<=\\frac{1}{n}$, then either $\\forall x_i=\\frac{i-1}{n})$ or $x_n = x_0 + \\sum_{i=1}^{i=n}(x_i - x_{i-1})>1$, both of which do not hold. Assume that $f(x)$ is indentity function, i.e. $f(x)=x$. Then\n",
    "\\begin{align}\n",
    "d_i(f,A^{opt}_n f) &= sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x)}{2}|\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} |x -\\frac{sup_{x\\in [x_i, x_{i+1}] }x+inf_{x\\in [x_i,x_{i+1})} x}{2}|\\\\ \n",
    "&= \\frac{||x_{j+1}-x_j||}{2}\\\\\n",
    "&\\geq \\frac{x_{j+1}-x_j}{2}\\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "Then consider the case $(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$. Let $f(x)=x$ and $x_i = \\frac{i-1}{n}$. Then if $\\forall j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) = \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}$, then inductively $\\forall j\\in\\{1\\ldots n\\}=\\frac{\\mathcal{X}(f, j)}{2} - \\sum^{i-1}_{j=1} w_j$, which cannot hold. Therefore $\\exists j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) \\neq \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}>x_j$<br>\n",
    "If $(A_n f)(x)>\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}$, then $|f(x_j)-(A_n f)(x_j)|=(A_n f)(x_j)-f(x_j)>\\frac{1}{2n}= \\frac{\\omega(id; \\frac{1}{n})}{2}$.<br>\n",
    "If $(A_n f)(x)<\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}<x_{j+1}=x_j + \\frac{1}{n}$, then \n",
    "\\begin{align}\n",
    "lim_{x\\rightarrow x_{j+1}}|f(x)-(A_n f)(x)|&=lim_{x\\rightarrow x_{j+1}}(x - (A_n f)(x) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j+1}) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j}) > x_{j+1} - (x_j  - \\frac{1}{2n}) \\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Approximation with the piecewise linear function**\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the piecewise linear function. Then $\\forall f\\in C([0,1]), \\forall\\epsilon>0:\\exists n\\in N, w_i, a_i, b_i \\in R, i\\in\\{0\\ldots n\\}$:\n",
    "$$(A_n f)(x)=\\sum^n_{i=1} w_i\\varphi(a_i x + b_i)$$\n",
    "as an approximation of the function $f(\\cdot)$; that is \n",
    "$$sup_{x\\in[0,1]}|f(x) - (A_n f)(x)|<\\epsilon$$\n",
    "Special Cases\n",
    "=============\n",
    "\n",
    "Prove the universal approximation theorem for the follwing 3 popular activation functions\n",
    "* threshold function\n",
    "* piecewise linear function\n",
    "* logistic function\n",
    "\n",
    "<h3>B-Splines</h3>\n",
    "\n",
    "B-splines are piecewise functions that can be translated and dilated from the mother functions \n",
    "\\begin{align}\n",
    "B_1(x) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "B_2 &=  B_1 * B_1=\\int^\\infty_\\infty B_1(y)B_1(x-y)dy=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1+x & \\quad -1\\leq x \\leq 0 \\\\\n",
    "            1-x & \\quad 0<x\\leq 1\\\\\n",
    "            0 & \\quad elsewhere\n",
    "        \\end{array}\n",
    "    \\right.\\\\   \n",
    "B_3 &=  B_1 * B_1 * B1\\\\\n",
    "\\ldots\n",
    "\\end{align}\n",
    "\n",
    "* $B_1$ is piecewise constant. \n",
    "* $B_2$ is piecewise linear \n",
    "* $B_3$ is piecewise quadratic, $\\ldots$.\n",
    "\n",
    "**Theory of splines**\n",
    "* Every continuous function in $R^n$ (by generalising B-spline to $R^n$ space) can be well approximated with the linear combination of this kind of functions.\n",
    "\n",
    "\n",
    "<h3>Threshold function as activation function</h3>\n",
    "\n",
    "$\n",
    "\\varphi(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\geq 0 \\\\\n",
    "            0 & \\quad x < 0\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$\n",
    "\n",
    "\n",
    "**Approximation with the threshold function**\n",
    "\n",
    "* Any arbitrary continuous function, defined on $[0,1]$ can be arbitrary well uniformly approximated by a multilayer feed-forward neural network with one hidden layer (that contains only finite number of neurons) using the threshold activation functions in the hidden layer and a linear neuron in the output layer.\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the ***threshold function***. Then $\\forall f\\in C([0,1]), \\forall \\epsilon>0:\\exists n\\in N, w_i, b_i\\in R, i\\in\\{0,\\ldots, n\\}:$\n",
    "$$(A_n f)(x)=\\sum^n_{i=1}w_i\\varphi(x+b_i)$$ \n",
    "as an approximation of function $f(\\cdot)$\n",
    "$$sup_{x\\in[0,1]}|(A_n f)(x)-f(x)|<\\epsilon$$\n",
    "\n",
    "><font color='green'>\n",
    "**Proof**<br>\n",
    "    Build the 1st order B-spline mother function (Haar function):\n",
    "    \\begin{equation}\n",
    "    \\Psi(x, 1, \\frac{1}{2})=\\varphi(x+\\frac{1}{2})-\\varphi(x-\\frac{1}{2})== \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    \\end{equation} \n",
    "    Obviously $\\Psi(x, 1, \\frac{1}{2})=B_1(x)$.                                             \n",
    "    Following B-spline theory, every continous function can be approximated by the translation and dilation of this function.\n",
    "    </font>\n",
    "\n",
    "If $n+1$ Haar functions are wanted, the simplest construction is\n",
    "$$(A_{n+1}f)(x)=\\sum^n_{i=0} w_i\\Psi_i(x, b), \\forall i\\in\\{0,\\ldots, n\\}:w_i=f(x_i), b= \\frac{1}{2n}$$\n",
    "Assume $x_i=\\frac{i}{n}, b=\\frac{1}{2n},\\Psi_i(x,b)=\\Psi(x-x_i, b), \\varphi_i(x)=\\varphi(x-x_i),\\forall i\\in\\{0\\ldots n\\}$, then **in the original paper, the equation $w_i\\Psi_i(x, b) = \\sum^n_{i=0} w_i(\\varphi_{i}(x-b)-\\varphi_{i}(x+b))$ conflicts with definition.** The correct transmission of $\\Psi_i(x, b)$ is as follows.\n",
    "><font color='green'>\n",
    "\\begin{align}\n",
    "\\Psi_i(x-b) &= \\Psi(x-x_i, b)\\\\\n",
    "&= \\varphi(x- x_i + b)-\\varphi(x-x_i-b)\\\\\n",
    "&= \\varphi(x - \\frac{i}{n} + \\frac{1}{2n}) - \\varphi(x - \\frac{i}{n} - \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - \\frac{i}{n}) + \\frac{1}{2n}) - \\varphi((x - \\frac{i + 1}{n}) + \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - x_{i}) + b) - \\varphi((x - x_{i+1}) + b)\\\\\n",
    "&= \\varphi_{i}(x+b)-\\varphi_{i+1}(x+b)\n",
    "\\end{align}\n",
    "Note that since $x\\in[0,1]$, $\\varphi_{n+1}(x+b)=\\varphi(x - 1 - \\frac{1}{2n})=0$. Then\n",
    "\\begin{align}\n",
    "\\sum^n_{i=0} w_i\\Psi_i(x, b) &= \\sum^n_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i}(x-b)) \\\\\n",
    "&= \\sum^{n}_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i+1}(x+b))\\\\\n",
    "&= w_0\\varphi_0(x+b) + \\sum^n_{i=1} (w_i-w_{i-1})\\varphi_i(x+b)\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "Therefore, $n+1$ neurons rather then $2(n+1)$ are needed. The weights $\\{w_i\\}_n$ for the compressed neurons can be represented as\n",
    "$$\n",
    "w_i' = \\left\\{\n",
    "        \\begin{array}\n",
    "            f(x_0) & \\quad i=0 \\\\\n",
    "            f(x_i) - \\sum^{i-1}_{j=0}w_j' & \\quad i \\in(0, n]\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$$\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    When $i=0$, $w_0'=w_0=f(x_0)$\n",
    "    <br>\n",
    "    When $i>0$, $w_i'=w_i-w_{i-1}=f(x_i)-f(x_{i-1})$. Then\n",
    "    \\begin{align}\n",
    "    \\sum^{i-1}_{j=0}w_j' &= w_0' + \\sum^{i-1}_{j=1}(w_j-w_{j-1}) \\\\\n",
    "    &= f(x_0) + \\sum^{i-1}_{j=1}(f(x_{j})-f(x_{j-1}))\\\\\n",
    "    &= f(x_{i-1})\\\\\n",
    "    &= f(x_i)-w_i'\n",
    "    \\end{align}\n",
    "    Therefore, $w_i'=f(x_i)-\\sum^{i-1}_{j=0}w_j'$. \n",
    " </font>\n",
    "\n",
    "Optimal approximation with biases not depending on $f$\n",
    "\\begin{align}\n",
    "x &= \\frac{i-1}{n}\\\\\n",
    "w_i^{opt} &= \\frac{\\mathcal{X}(f,i)}{2}-\\sum_{j=1}^{i-1}w_j\\\\\n",
    "\\mathcal{X}(f,i)&= sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x) \\\\\n",
    "(A_n^{opt} f)(x)&=\\sum^n_{i=1} w_i^{opt}\\varphi(x-x_i)\n",
    "\\end{align}\n",
    "\n",
    "To estimate the error of the approximation, define the modulus of continuity:\n",
    "$$\\omega(f;\\delta)=sup\\{|f(x)-f(y)|:x,y\\in D_f\\wedge |x-y|\\leq \\delta\\}$$\n",
    "\n",
    "* If $f$ is continuous and $lim_{n\\rightarrow\\infty}a_n=0$, then $lim_{n\\rightarrow\\infty}\\omega(f;a_n)=0$\n",
    "* $\\forall \\epsilon >0:\\omega(id;\\epsilon)=\\epsilon$ where $id(x)=x$ is identity function\n",
    "\n",
    "**Theorem**: $\\forall n\\in N:\\forall f\\in[0,1]\\rightarrow R:||f-A_n^{opt}f||_\\infty\\leq\\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "For an $x\\in[x_i, x_{i+1}]$,\n",
    "\\begin{array}\n",
    "(A_n^{opt}f(x) &= \\sum^n_{j=1}w_j^{opt}\\varphi(x-x_j) \\\\\n",
    "&=  \\sum^i_{j=1}w_j^{opt}\\\\\n",
    "&= w_i^{opt} +  \\sum^{i-1}_{j=1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}-\\sum^{i-1}_{j=1}w_j^{opt} + \\sum_{j=1}^{i-1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}\n",
    "\\end{array}\n",
    "\n",
    "Then formulate the error of the approximation in $[x_i, x_{i+1})$ by\n",
    "\n",
    "$$\n",
    "d_i(f,A^{opt}_n f) = sup_{x\\in[x_i, x_{i+1})} |f(x)-(A_n^{opt}f)(x)|\n",
    "=sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{\\mathcal{X}(f, i)}{2}| \n",
    "$$\n",
    "\n",
    "Because\n",
    "\n",
    "\\begin{array}\n",
    "\\\\\\omega(f;\\frac{1}{n})&=sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\}\\\\\n",
    "&\\geq sup\\{|f(x)-f(y)|:x,y\\in[x_i, x_{i+1})\\}\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} f(x) - inf_{y\\in[x_i, x_{i+1})}f(y)\\\\\n",
    "&= 2\\cdot (sup_{x\\in[x_i, x_{i+1})}f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2})\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2}|\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x)-\\frac{\\mathcal{X}(f, i)}{2}|\\\\\n",
    "&= 2\\cdot d_i(f,A^{opt}_n f)\n",
    "\\end{array}\n",
    "\n",
    "Since $\\omega(f;\\frac{1}{n})$ does not depend on $i$, $||f-A^{opt}_n f||_\\infty=max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)\\leq \\frac{\\omega(f;\\frac{1}{n})}{2}$.\n",
    "\n",
    "**Corollary for convergence**: If $f$ is continuous then $lim_{n\\rightarrow \\infty}||f-A_n^{opt} f||_\\infty = 0$.\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    \\begin{align}\n",
    "    lim_{n\\rightarrow\\infty}||f-A_n^{opt}f||_\\infty &=  lim_{n\\rightarrow\\infty}|| max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)||_\\infty \\\\\n",
    "    & \\leq  lim_{n\\rightarrow\\infty}\\frac{\\omega(f;\\frac{1}{n})}{2} \\\\\n",
    "    & = lim_{n\\rightarrow\\infty} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\} \\\\\n",
    "    & = lim_{\\delta\\rightarrow 0} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\delta\\}\\\\\n",
    "    & = 0\\ \\text{ due to continuity of $f(x)$}\n",
    "    \\end{align}\n",
    "    \n",
    " </font>\n",
    "\n",
    "**Theorem for optimality**: $A^{opt}_n f$ is optimal in the sense that if the biases are independent from $f$, then if \n",
    "$(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})\\vee(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$\n",
    "then $\\exists f\\in[0,1]\\rightarrow R:||f-A_nf||_\\infty > \\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "Firstly consider the case $(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})$. If $\\forall j\\in\\{0,\\ldots, n\\}: x_{j+1}-x_j<=\\frac{1}{n}$, then either $\\forall x_i=\\frac{i-1}{n})$ or $x_n = x_0 + \\sum_{i=1}^{i=n}(x_i - x_{i-1})>1$, both of which do not hold. Assume that $f(x)$ is indentity function, i.e. $f(x)=x$. Then\n",
    "\\begin{align}\n",
    "d_i(f,A^{opt}_n f) &= sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x)}{2}|\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} |x -\\frac{sup_{x\\in [x_i, x_{i+1}] }x+inf_{x\\in [x_i,x_{i+1})} x}{2}|\\\\ \n",
    "&= \\frac{||x_{j+1}-x_j||}{2}\\\\\n",
    "&\\geq \\frac{x_{j+1}-x_j}{2}\\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "Then consider the case $(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$. Let $f(x)=x$ and $x_i = \\frac{i-1}{n}$. Then if $\\forall j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) = \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}$, then inductively $\\forall j\\in\\{1\\ldots n\\}=\\frac{\\mathcal{X}(f, j)}{2} - \\sum^{i-1}_{j=1} w_j$, which cannot hold. Therefore $\\exists j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) \\neq \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}>x_j$<br>\n",
    "If $(A_n f)(x)>\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}$, then $|f(x_j)-(A_n f)(x_j)|=(A_n f)(x_j)-f(x_j)>\\frac{1}{2n}= \\frac{\\omega(id; \\frac{1}{n})}{2}$.<br>\n",
    "If $(A_n f)(x)<\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}<x_{j+1}=x_j + \\frac{1}{n}$, then \n",
    "\\begin{align}\n",
    "lim_{x\\rightarrow x_{j+1}}|f(x)-(A_n f)(x)|&=lim_{x\\rightarrow x_{j+1}}(x - (A_n f)(x) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j+1}) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j}) > x_{j+1} - (x_j  - \\frac{1}{2n}) \\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Approximation with the piecewise linear function**\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the piecewise linear function. Then $\\forall f\\in C([0,1]), \\forall\\epsilon>0:\\exists n\\in N, w_i, a_i, b_i \\in R, i\\in\\{0\\ldots n\\}$:\n",
    "$$(A_n f)(x)=\\sum^n_{i=1} w_i\\varphi(a_i x + b_i)$$\n",
    "as an approximation of the function $f(\\cdot)$; that is \n",
    "$$sup_{x\\in[0,1]}|f(x) - (A_n f)(x)|<\\epsilon$$\n",
    "\n",
    "Special Cases\n",
    "=============\n",
    "\n",
    "Prove the universal approximation theorem for the follwing 3 popular activation functions\n",
    "* threshold function\n",
    "* piecewise linear function\n",
    "* logistic function\n",
    "\n",
    "<h3>B-Splines</h3>\n",
    "\n",
    "B-splines are piecewise functions that can be translated and dilated from the mother functions \n",
    "\\begin{align}\n",
    "B_1(x) &= \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "B_2 &=  B_1 * B_1=\\int^\\infty_\\infty B_1(y)B_1(x-y)dy=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1+x & \\quad -1\\leq x \\leq 0 \\\\\n",
    "            1-x & \\quad 0<x\\leq 1\\\\\n",
    "            0 & \\quad elsewhere\n",
    "        \\end{array}\n",
    "    \\right.\\\\   \n",
    "B_3 &=  B_1 * B_1 * B1\\\\\n",
    "\\ldots\n",
    "\\end{align}\n",
    "\n",
    "* $B_1$ is piecewise constant. \n",
    "* $B_2$ is piecewise linear \n",
    "* $B_3$ is piecewise quadratic, $\\ldots$.\n",
    "\n",
    "**Theory of splines**\n",
    "* Every continuous function in $R^n$ (by generalising B-spline to $R^n$ space) can be well approximated with the linear combination of this kind of functions.\n",
    "\n",
    "\n",
    "<h3>Threshold function as activation function</h3>\n",
    "\n",
    "$\n",
    "\\varphi(x) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\geq 0 \\\\\n",
    "            0 & \\quad x < 0\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Approximation with the threshold function**\n",
    "\n",
    "* Any arbitrary continuous function, defined on $[0,1]$ can be arbitrary well uniformly approximated by a multilayer feed-forward neural network with one hidden layer (that contains only finite number of neurons) using the threshold activation functions in the hidden layer and a linear neuron in the output layer.\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the ***threshold function***. Then $\\forall f\\in C([0,1]), \\forall \\epsilon>0:\\exists n\\in N, w_i, b_i\\in R, i\\in\\{0,\\ldots, n\\}:$\n",
    "$$(A_n f)(x)=\\sum^n_{i=1}w_i\\varphi(x+b_i)$$ \n",
    "as an approximation of function $f(\\cdot)$\n",
    "$$sup_{x\\in[0,1]}|(A_n f)(x)-f(x)|<\\epsilon$$\n",
    "\n",
    "><font color='green'>\n",
    "**Proof**<br>\n",
    "    Build the 1st order B-spline mother function (Haar function):\n",
    "    \\begin{equation}\n",
    "    \\Psi(x, 1, \\frac{1}{2})=\\varphi(x+\\frac{1}{2})-\\varphi(x-\\frac{1}{2})== \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad -\\frac{1}{2}\\leq x < \\frac{1}{2} \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "    \\end{equation} \n",
    "    Obviously $\\Psi(x, 1, \\frac{1}{2})=B_1(x)$.                                             \n",
    "    Following B-spline theory, every continous function can be approximated by the translation and dilation of this function.\n",
    "    </font>\n",
    "\n",
    "If $n+1$ Haar functions are wanted, the simplest construction is\n",
    "$$(A_{n+1}f)(x)=\\sum^n_{i=0} w_i\\Psi_i(x, b), \\forall i\\in\\{0,\\ldots, n\\}:w_i=f(x_i), b= \\frac{1}{2n}$$\n",
    "Assume $x_i=\\frac{i}{n}, b=\\frac{1}{2n},\\Psi_i(x,b)=\\Psi(x-x_i, b), \\varphi_i(x)=\\varphi(x-x_i),\\forall i\\in\\{0\\ldots n\\}$, the transmission of $\\Psi_i(x, b)$ is as follows.\n",
    "><font color='green'>\n",
    "\\begin{align}\n",
    "\\Psi_i(x-b) &= \\Psi(x-x_i, b)\\\\\n",
    "&= \\varphi(x- x_i + b)-\\varphi(x-x_i-b) \\\\\n",
    "&= \\varphi(x - \\frac{i}{n} + \\frac{1}{2n}) - \\varphi(x - \\frac{i}{n} - \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - \\frac{i}{n}) + \\frac{1}{2n}) - \\varphi((x - \\frac{i + 1}{n}) + \\frac{1}{2n}) \\\\\n",
    "&= \\varphi((x - x_{i}) + b) - \\varphi((x - x_{i+1}) + b)\\\\\n",
    "&= \\varphi_{i}(x+b)-\\varphi_{i+1}(x+b)\n",
    "\\end{align}\n",
    "Note that in the original paper, the equation $w_i\\Psi_i(x, b) = \\sum^n_{i=0} w_i(\\varphi_{i}(x-b)-\\varphi_{i}(x+b))$ conflicts with definition.<br>\n",
    "Since $x\\in[0,1]$, $\\varphi_{n+1}(x+b)=\\varphi(x - 1 - \\frac{1}{2n})=0$. Then\n",
    "\\begin{align}\n",
    "\\sum^n_{i=0} w_i\\Psi_i(x, b) &= \\sum^n_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i}(x-b)) \\\\\n",
    "&= \\sum^{n}_{i=0} w_i(\\varphi_{i}(x+b)-\\varphi_{i+1}(x+b))\\\\\n",
    "&= w_0\\varphi_0(x+b) + \\sum^n_{i=1} (w_i-w_{i-1})\\varphi_i(x+b)\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "Therefore, $n+1$ neurons rather then $2(n+1)$ are needed. The weights $\\{w_i\\}_n$ for the compressed neurons can be represented as\n",
    "$$\n",
    "w_i' = \\left\\{\n",
    "        \\begin{array}\n",
    "            f(x_0) & \\quad i=0 \\\\\n",
    "            f(x_i) - \\sum^{i-1}_{j=0}w_j' & \\quad i \\in(0, n]\n",
    "        \\end{array}\n",
    "    \\right.\\\\\n",
    "$$\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    When $i=0$, $w_0'=w_0=f(x_0)$\n",
    "    <br>\n",
    "    When $i>0$, $w_i'=w_i-w_{i-1}=f(x_i)-f(x_{i-1})$. Then\n",
    "    \\begin{align}\n",
    "    \\sum^{i-1}_{j=0}w_j' &= w_0' + \\sum^{i-1}_{j=1}(w_j-w_{j-1}) \\\\\n",
    "    &= f(x_0) + \\sum^{i-1}_{j=1}(f(x_{j})-f(x_{j-1}))\\\\\n",
    "    &= f(x_{i-1})\\\\\n",
    "    &= f(x_i)-w_i'\n",
    "    \\end{align}\n",
    "    Therefore, $w_i'=f(x_i)-\\sum^{i-1}_{j=0}w_j'$. \n",
    " </font>\n",
    "\n",
    "Optimal approximation with biases not depending on $f$\n",
    "\\begin{align}\n",
    "x &= \\frac{i-1}{n}\\\\\n",
    "w_i^{opt} &= \\frac{\\mathcal{X}(f,i)}{2}-\\sum_{j=1}^{i-1}w_j\\\\\n",
    "\\mathcal{X}(f,i)&= sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x) \\\\\n",
    "(A_n^{opt} f)(x)&=\\sum^n_{i=1} w_i^{opt}\\varphi(x-x_i)\n",
    "\\end{align}\n",
    "\n",
    "To estimate the error of the approximation, define the modulus of continuity:\n",
    "$$\\omega(f;\\delta)=sup\\{|f(x)-f(y)|:x,y\\in D_f\\wedge |x-y|\\leq \\delta\\}$$\n",
    "\n",
    "* If $f$ is continuous and $lim_{n\\rightarrow\\infty}a_n=0$, then $lim_{n\\rightarrow\\infty}\\omega(f;a_n)=0$\n",
    "* $\\forall \\epsilon >0:\\omega(id;\\epsilon)=\\epsilon$ where $id(x)=x$ is identity function\n",
    "\n",
    "**Theorem**: $\\forall n\\in N:\\forall f\\in[0,1]\\rightarrow R:||f-A_n^{opt}f||_\\infty\\leq\\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "For an $x\\in[x_i, x_{i+1}]$,\n",
    "\\begin{array}\n",
    "(A_n^{opt}f(x) &= \\sum^n_{j=1}w_j^{opt}\\varphi(x-x_j) \\\\\n",
    "&=  \\sum^i_{j=1}w_j^{opt}\\\\\n",
    "&= w_i^{opt} +  \\sum^{i-1}_{j=1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}-\\sum^{i-1}_{j=1}w_j^{opt} + \\sum_{j=1}^{i-1}w_j^{opt}\\\\\n",
    "&= \\frac{\\mathcal{X}(f,i)}{2}\n",
    "\\end{array}\n",
    "\n",
    "Then formulate the error of the approximation in $[x_i, x_{i+1})$ by\n",
    "\n",
    "$$\n",
    "d_i(f,A^{opt}_n f) = sup_{x\\in[x_i, x_{i+1})} |f(x)-(A_n^{opt}f)(x)|\n",
    "=sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{\\mathcal{X}(f, i)}{2}| \n",
    "$$\n",
    "\n",
    "Because\n",
    "\n",
    "\\begin{array}\n",
    "\\\\\\omega(f;\\frac{1}{n})&=sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\}\\\\\n",
    "&\\geq sup\\{|f(x)-f(y)|:x,y\\in[x_i, x_{i+1})\\}\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} f(x) - inf_{y\\in[x_i, x_{i+1})}f(y)\\\\\n",
    "&= 2\\cdot (sup_{x\\in[x_i, x_{i+1})}f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2})\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x) - \\frac{sup_{x\\in[x_i, x_{i+1})} f(x) + inf_{y\\in[x_i, x_{i+1})}f(y)}{2}|\\\\\n",
    "&= 2\\cdot sup_{x\\in[x_i, x_{i+1})}|f(x)-\\frac{\\mathcal{X}(f, i)}{2}|\\\\\n",
    "&= 2\\cdot d_i(f,A^{opt}_n f)\n",
    "\\end{array}\n",
    "\n",
    "Since $\\omega(f;\\frac{1}{n})$ does not depend on $i$, $||f-A^{opt}_n f||_\\infty=max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)\\leq \\frac{\\omega(f;\\frac{1}{n})}{2}$.\n",
    "\n",
    "**Corollary for convergence**: If $f$ is continuous then $lim_{n\\rightarrow \\infty}||f-A_n^{opt} f||_\\infty = 0$.\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "    \\begin{align}\n",
    "    lim_{n\\rightarrow\\infty}||f-A_n^{opt}f||_\\infty &=  lim_{n\\rightarrow\\infty}|| max_{i\\in\\{1\\ldots n\\}} d_i(f,A^{opt}_n f)||_\\infty \\\\\n",
    "    & \\leq  lim_{n\\rightarrow\\infty}\\frac{\\omega(f;\\frac{1}{n})}{2} \\\\\n",
    "    & = lim_{n\\rightarrow\\infty} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\frac{1}{n}\\} \\\\\n",
    "    & = lim_{\\delta\\rightarrow 0} sup\\{|f(x)-f(y)|:x,y\\in[0,1]\\wedge|x-y|\\leq \\delta\\}\\\\\n",
    "    & = 0\\ \\text{ due to continuity of $f(x)$}\n",
    "    \\end{align}\n",
    "    \n",
    " </font>\n",
    "\n",
    "**Theorem for optimality**: $A^{opt}_n f$ is optimal in the sense that if the biases are independent from $f$, then if \n",
    "$(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})\\vee(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$\n",
    "then $\\exists f\\in[0,1]\\rightarrow R:||f-A_nf||_\\infty > \\frac{\\omega(f;\\frac{1}{n})}{2}$\n",
    "\n",
    ">**Proof**<br>\n",
    "Firstly consider the case $(\\exists i\\in\\{1\\ldots n\\}:x_i\\neq \\frac{i-1}{n})$. If $\\forall j\\in\\{0,\\ldots, n\\}: x_{j+1}-x_j<=\\frac{1}{n}$, then either $\\forall x_i=\\frac{i-1}{n})$ or $x_n = x_0 + \\sum_{i=1}^{i=n}(x_i - x_{i-1})>1$, both of which do not hold. Assume that $f(x)$ is indentity function, i.e. $f(x)=x$. Then\n",
    "\\begin{align}\n",
    "d_i(f,A^{opt}_n f) &= sup_{x\\in[x_i, x_{i+1})} |f(x)-\\frac{sup_{x\\in [x_i, x_{i+1}] }f(x)+inf_{x\\in [x_i,x_{i+1})} f(x)}{2}|\\\\\n",
    "&= sup_{x\\in[x_i, x_{i+1})} |x -\\frac{sup_{x\\in [x_i, x_{i+1}] }x+inf_{x\\in [x_i,x_{i+1})} x}{2}|\\\\ \n",
    "&= \\frac{||x_{j+1}-x_j||}{2}\\\\\n",
    "&\\geq \\frac{x_{j+1}-x_j}{2}\\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "Then consider the case $(\\exists i\\in\\{1\\ldots n\\}: w_i\\neq \\frac{\\mathcal{X}(f, i)}{2} - \\sum^{i-1}_{j=1} w_j)$. Let $f(x)=x$ and $x_i = \\frac{i-1}{n}$. Then if $\\forall j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) = \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}$, then inductively $\\forall j\\in\\{1\\ldots n\\}=\\frac{\\mathcal{X}(f, j)}{2} - \\sum^{i-1}_{j=1} w_j$, which cannot hold. Therefore $\\exists j\\in\\{0,\\ldots, n\\}: \\forall x\\in[x_j,x_{j+1}): (A_n f)(x) \\neq \\frac{\\mathcal{X}(f,j)}{2}=x_j + \\frac{1}{2n}>x_j$<br>\n",
    "If $(A_n f)(x)>\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}$, then $|f(x_j)-(A_n f)(x_j)|=(A_n f)(x_j)-f(x_j)>\\frac{1}{2n}= \\frac{\\omega(id; \\frac{1}{n})}{2}$.<br>\n",
    "If $(A_n f)(x)<\\frac{\\mathcal{X}(f,x_j)}{2}=x_j + \\frac{1}{2n}<x_{j+1}=x_j + \\frac{1}{n}$, then \n",
    "\\begin{align}\n",
    "lim_{x\\rightarrow x_{j+1}}|f(x)-(A_n f)(x)|&=lim_{x\\rightarrow x_{j+1}}(x - (A_n f)(x) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j+1}) \\\\\n",
    "&= x_{j+1} - (A_n f)(x_{j}) > x_{j+1} - (x_j  - \\frac{1}{2n}) \\\\\n",
    "&> \\frac{1}{2n}\\\\\n",
    "&= \\frac{\\omega(id; \\frac{1}{n})}{2}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Approximation with the piecewise linear function**\n",
    "\n",
    "* Let $\\varphi(\\cdot)$ be the piecewise linear function. Then $\\forall f\\in C([0,1]), \\forall\\epsilon>0:\\exists n\\in N, w_i, a_i, b_i \\in R, i\\in\\{0\\ldots n\\}$:\n",
    "$$(A_n f)(x)=\\sum^n_{i=1} w_i\\varphi(a_i x + b_i)$$\n",
    "as an approximation of the function $f(\\cdot)$; that is \n",
    "$$sup_{x\\in[0,1]}|f(x) - (A_n f)(x)|<\\epsilon$$\n",
    "\n",
    ">**Proof**<br>\n",
    "Build mother function $\\Psi(x, 1, \\frac{1}{2})=\\varphi(x+\\frac{1}{2})-\\varphi(x-\\frac{1}{2})=B_2(x) =  B_1 * B_1=\\int^\\infty_\\infty B_1(y)B_1(x-y)dy=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1+x & \\quad -1\\leq x \\leq 0 \\\\\n",
    "            1-x & \\quad 0<x\\leq 1\\\\\n",
    "            0 & \\quad elsewhere\n",
    "        \\end{array}\n",
    "    \\right.\\\\   $\n",
    "And $B_2$ functions can uniformly approximate any arbitrary continuous function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximation with the logistic function**\n",
    "\n",
    "* An arbitrary continuous function, defined on $[0,1]$ can be arbitrary well uniformly approximated by a multilayer feed-forward neural network with one hidden layer using the logistic function as activation function ($\\varphi$)\n",
    "\n",
    "* In the original thesis, an idea of proof was given.\n",
    "\n",
    "><font color='green'>\n",
    " **Proof**\n",
    "    <br>\n",
    "\\begin{align}\n",
    "    \\Psi(x,a,b) &= \\varphi(ax+b)-\\varphi(ax-b)\\\\\n",
    "    &= \\frac{1}{1 + exp(-ax-b)} - \\frac{1}{1+exp(-ax+b)} \\\\\n",
    "    &= \\frac{(exp(b)-exp(-b))exp(ax)}{(1+exp(ax+b))(1+exp(ax-b))}\\\\ \n",
    "    &=\\Psi(-x,a,b)\n",
    "\\end{align}\n",
    "Obviously, $\\Psi(x,a,b)$ is like a bell and is monotonous on each side of $x=0$.\n",
    "$$lim_{x\\rightarrow \\pm\\infty}\\Psi(x,a,b)=0$$\n",
    "$$\\max{\\Psi(x,a,b)}= lim_{x\\rightarrow 0}\\Psi(x,a,b)=\\frac{exp(2b)-1}{(1+exp(b))^2}$$\n",
    "Let $\\Psi_i(x,a)=\\Psi(x-x_i, a, b)$ where $x_i=\\frac{i}{n}$ and $b\\neq 0$ is some constant. The target is to find vectors of $w$ and $a$ such that:\n",
    "\\begin{array}\n",
    "    (A_{2(n+1)}f)(w,a,x)=\\sum^n_{j=1} w_j\\Psi_j(x,a_j)\\\\\n",
    "    \\forall i\\in\\{0,\\ldots, n\\}: (A_{2(n+1)} f)(w,a,x_i) = f(x_i)\n",
    "\\end{array}   \n",
    "Define the following equations\n",
    "\\begin{array}\n",
    "    \\\\F = [f(x_0), f(x_1),\\ldots, f(x_n)]^T\\\\\n",
    "    w = [w_0, w_1, \\ldots, w_n]^T\\\\\n",
    "    G(a) = \\begin{bmatrix}\n",
    "    \\Psi_0(x_0, a_0) & \\Psi_1(x_0, a_1) & \\ldots & \\Psi_n(x_0, a_n)\\\\\n",
    "    \\Psi_0(x_1, a_0) & \\Psi_1(x_1, a_1) & \\ldots & \\Psi_n(x_1, a_n)\\\\\n",
    "    \\vdots           & \\vdots           & \\vdots & \\vdots          \\\\\n",
    "    \\Psi_0(x_n, a_0) & \\Psi_1(x_n, a_1) & \\ldots & \\Psi_n(x_n, a_n)\n",
    "\\end{bmatrix}\\\\\n",
    "    G(a) w = F\\\\\n",
    "    w =  G(a)^{-1} F\n",
    "\\end{array}\n",
    "**According to **Gershgorin's theorem**, if $\\forall i\\in\\{0\\ldots n\\}:\\sum_{j!=i}\\Psi_i(x_j,a_i)<\\Psi_i(x_i, a_i)$, then $G(a)^{-1}$ exists.??????????**<br>\n",
    "**Since $\\Psi_i(x_i, a_i)$ does not depend on $a_i$ and it is equal to $\\Psi_0(0)=\\varphi(b)-\\varphi(-b)$ ??????????, then the target is to find suitable $a$ to make the inequality holds.**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
