{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"http://papers.nips.cc/paper/4013-policy-gradients-in-linearly-solvable-mdps\">\n",
    "Policy Gradient in Linearly-Solvable MDPs</a></h1>\n",
    "by Emanuel Todorov et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "=======\n",
    "* Policy gradient within the framework of linearly-solvable MDPs for both discrete and continuous stochastic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearly-solvable MDP (LMDP)\n",
    "============\n",
    "\n",
    "Define a `state cost` $q(x)$ over a assumably `discrete state space` $X$. \n",
    "\n",
    "Define `passive dynamics` as $p(x'|x)$ and transition probability under `admissible action` as $\\pi(x'|x)$, then $$p(x'|x)=0\\implies \\pi(x'|x)=0$$\n",
    "\n",
    "The `cost function` is $l(x, \\pi(|x))=q(x)+D_{KL}(\\pi(|x)||p(|x))=q(x)+\\sum_{x'}[\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)})]$\n",
    "\n",
    "The `average cost` $c$ and `differential cost-to-go` v(x) for $\\pi$ satisfies\n",
    "$$c+v(x)=q(x)+\\sum_{x'}\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))$$\n",
    ">Note: if $v':X\\rightarrow R$ is a solution for $v(x)$, then $v'+C$ is also a solution where $C$ is any constant.\n",
    "\n",
    "Assume $v^*$ is the optimal solution and the corresponding action-induced transition is $\\pi^*$. Then\n",
    "$$c+v^*(x)=q(x)+\\sum_{x'}\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v^*(x'))$$\n",
    "$$\\pi^*(x'|x)=\\frac{p(x'|x)exp(-v^*(x'))}{\\sum_y p(y|x)exp(-v^*(y))}$$\n",
    "\n",
    "><font color='green'>\n",
    "<a href=\"http://localhost:8889/notebooks/Efficient%20Computation%20of%20Optimal%20Actions.ipynb\">**Proof**</a> can be found in <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/19574462\">Efficient Computation of Optimal Actions</a> by Emanuel Todorov\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Discrete Problems: Policy Gradient Method \n",
    "===================\n",
    "\n",
    "<h3>Policy gradient for general parameterization</h3>\n",
    "\n",
    "Consider parameterized transition probability $\\pi(x'|x, w)$. \n",
    "\n",
    "Define the probability of reaching state $x$ as $\\mu(x,w)$ and the probability of reaching $x'$ after reaching $x$ is $\\mu(x, x', w)=\\mu(x,w)\\pi(x'|x,w)$. **Note: discount factor is not considered.**\n",
    "\n",
    "**`Policy gradient`**: solve the optimal parameter $w$ that minizes the average cost via gradient descend.\n",
    "\n",
    "$$\\nabla_w c=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)}+v(x'))$$\n",
    "\n",
    "> <font color='green'>\n",
    "**Proof Sketch**\n",
    "(refer to <a href=\"https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\">policy gradient template</a>)<br>\n",
    "\\begin{align}\n",
    "c &=q(x)+\\sum_{x'}\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))-v(x)\\\\\n",
    "\\nabla_w c &=\\nabla_w[q(x)+\\sum_{x'}\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))-v(x)]\\\\\n",
    "&=\\nabla_w [\\sum_{x'}\\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))-v(x)]\\\\ \n",
    "&=\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))+\\sum_{x'}\\pi(x'|x)\\nabla_w (log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))-\\nabla_w v(x)\\\\\n",
    "\\nabla_w c &=\\nabla_w c\\sum_x\\mu(x)=\\sum_x\\mu(x)\\nabla_w c\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))+\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)\\nabla_w (log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))-\\sum_x\\mu(x)\\nabla_w v(x)\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))+\\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)\\nabla_w log\\frac{\\pi(x'|x)}{p(x'|x)} + \\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)\\nabla_w v(x')-\\sum_x\\mu(x)\\nabla_w v(x)\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x')) + \\sum_{x}\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)+ \\sum_{x'}\\mu(x')\\nabla_w v(x')-\\sum_x\\mu(x)\\nabla_w v(x)\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x')) + \\sum_{x}\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x')) + \\sum_{x}\\mu(x)\\nabla_w \\sum_{x'}\\pi(x'|x)\\\\\n",
    "&=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "\n",
    "<h3>Parameterize the policy</h3>\n",
    "\n",
    "Use Gibbs distribution for parameterized transition probability $\\pi(x'|x, w)\\triangleq \\frac{p(x'|x)exp(-w^T f(x'))}{\\sum_y p(y|x)exp(-w^T f(y))}$.\n",
    "\n",
    "Use operator $\\Pi[f](x)\\triangleq \\sum_y \\pi(y|x)f(y)$ and rewrite $\\nabla_w c=\\sum_{x,x'}\\mu(x, x')(\\Pi[f](x)-f(x'))(v(x')-w^T f(x'))$\n",
    "><font color='green'>\n",
    "**Proof Sketch**\n",
    "<br>\n",
    "\\begin{align}\n",
    "\\nabla_w c &=\\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\frac{\\pi(x'|x)}{p(x'|x)} + v(x'))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log\\pi(x'|x) - logp(x'|x) + v(x'))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(log p(x'|x) - w^Tf(x)-log\\sum_yp(y|x)exp(-w^T f(y)) - logp(x'|x) + v(x'))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(- w^Tf(x)+ v(x')-log\\sum_yp(y|x)exp(-w^T f(y)) )\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(- w^Tf(x)+ v(x'))+ \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)log\\sum_yp(y|x)exp(-w^T f(y)))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\pi(x'|x)(- w^Tf(x)+ v(x'))+ \\sum_x\\mu(x)(log\\sum_yp(y|x)exp(-w^T f(y))))\\sum_{x'}\\nabla_w \\pi(x'|x)\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\nabla_w \\frac{p(x'|x)exp(-w^T f(x'))}{\\sum_y p(y|x)exp(-w^T f(y))}(- w^Tf(x)+ v(x'))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\frac{p(x'|x)exp(-w^T f(x'))}{\\sum_y p(y|x)exp(-w^T f(y))}(f(x')-\\frac{\\sum_y -p(y|x)f(y)}{\\sum_y p(y|x)exp(-w^T f(y))})(- w^Tf(x)+ v(x'))\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)(-f(x')-\\frac{\\sum_y -p(y|x)f(y)}{\\sum_y p(y|x)exp(-w^T f(y))})(- w^Tf(x)+ v(x'))\\\\\n",
    "&= \\sum_{x,x}\\mu(x, x')(-f(x')+\\sum_y\\pi(y|x)f(y))(- w^Tf(x)+ v(x'))\\\\\n",
    "&= \\sum_{x,x'}\\mu(x, x')(\\Pi[f](x)-f(x'))(v(x')-w^T f(x'))\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "<h3>Compatible cost-to-go function approximation</h3>\n",
    "\n",
    "Approximate the cost-to-go function $v(x)$ with a compatible function which\n",
    "* has the same gradient as Q-function\n",
    "* is orghogonal to the remaining terms in the policy gradient\n",
    "\n",
    "Try $v(x)\\approx\\hat{\\\\v}(x)=r^Tf(x)$and define $\\epsilon_r(x)\\triangleq v(x)-\\hat{\\\\v}(x)$ and $d(r)\\triangleq \\sum_{x,x'}\\mu(x,x')(\\Pi[f](x)-f(x'))\\epsilon_r(x')$.\n",
    "It is provable that\n",
    "$$d(r)= \\sum_{x}\\mu(x)(\\Pi[f](x)\\Pi[\\epsilon_r](x)-f(x)\\epsilon_r(x))$$\n",
    "><font color='green'>\n",
    "**Proof Sketch**\n",
    "<br>\n",
    "\\begin{align}\n",
    "d(r) &=\\sum_{x,x'}\\mu(x,x')(\\Pi[f](x)-f(x'))\\epsilon_r(x')\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)(\\sum_{y}\\pi(y|x)f(y)-f(x'))(v(x')-r^Tf(x'))\\\\ \n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)\\sum_{y}\\pi(y|x)f(y)v(x')-\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')v(x')-\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)\\sum_{y}\\pi(y|x)f(y)r^Tf(x')+\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')r^Tf(x')\\\\\n",
    "&= \\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')\\sum_{y}\\pi(y|x)v(y)-\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')\\sum_{y}\\pi(y|x)r^Tf(y)-\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')v(x')+\\sum_x\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')r^Tf(x'))\\\\\n",
    "&= \\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')\\sum_{y}\\pi(y|x)(v(y)-r^Tf(y))- \\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')(v(x')-r^Tf(x'))\\\\\n",
    "&= \\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')\\sum_{y}\\pi(y|x)(v(y)-r^Tf(y))- \\sum_{x}\\mu(x)f(x)(v(x)-r^Tf(x))\\\\\n",
    "&= \\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)f(x')\\sum_{y}\\pi(y|x)\\epsilon_r(y)- \\sum_{x}\\mu(x)f(x)\\epsilon_r(x)\\\\  \n",
    "&= \\sum_{x}\\mu(x)\\Pi[f](x)\\Pi[\\epsilon_r](x)-\\sum_{x}\\mu(x)f(x)\\epsilon_r(x)\\\\\n",
    "&= \\sum_{x}\\mu(x)(\\Pi[f](x)\\Pi[\\epsilon_r](x)-f(x)\\epsilon_r(x))\n",
    "\\end{align}\n",
    "</font>\n",
    "\n",
    "Use least-square to minimize the error by weighting $f(x)$ of all states with $\\mu(x)$. When optimality $r_{LS}\\triangleq argmin_r\\ \\sum_x\\mu(x)(v(x)-r^Tf(x))^2$ is achieved, ideally the gradient w.r.t $r$ is $0$, thus\n",
    "$$\\nabla_r\\sum_x\\mu(x)(v(x)-r^Tf(x))^2=0\\implies\\text{f(x) and $\\epsilon_r(x)=v(x)-r^Tf(x)$ are orthorgonal}$$\n",
    "\n",
    "Rewrite $d(r)$ as linear combination\n",
    "$$d(r)=Ar-k$$\n",
    "$$A\\triangleq \\sum_x\\mu(x)(f(x)f(x)^T-\\pi[f](x)\\pi[f](x)^T)$$\n",
    "$$k\\triangleq \\sum_x\\mu(x)(f(x)v(x)-\\Pi[f](x)\\Pi[v](x))$$\n",
    "\n",
    "* **$A$ does not depend on $v(x), r$, but $k$ does. Now eleminate the influence of $v(x)$ on $k$**.\n",
    "\n",
    "Recall that $c+v(x)=l(x) + \\sum_x \\pi(x'|x)v(x')$. Then $k$ can be rewritten as\n",
    "$$k= \\sum_x\\mu(x)(g(x)v(x)-\\Pi[f](x)(l(x)-c)$$\n",
    "$$\\text{where }g(x)\\triangleq f(x)-\\Pi[f](x)$$\n",
    "\n",
    "Use least square to fit $\\tilde{\\\\v}(x)=s^Tg(x)$ to $v(x)$ as done to $f(x)$, then **$g(x)$ and $\\tilde{\\\\\\epsilon}_s(x)=v(x)-\\tilde{\\\\v}(x)$ are orthorgonal.** Then\n",
    "\\begin{align}\n",
    "k&= \\sum_x\\mu(x)(g(x)v(x)-\\Pi[f](x)(l(x)-c)\\\\\n",
    "&= \\sum_x\\mu(x)(g(x)(v(x)-\\tilde{\\\\\\epsilon}_s(x))-\\Pi[f](x)(l(x)-c)\\\\\n",
    "&= \\sum_x\\mu(x)(g(x)\\tilde{\\\\v}(x)-\\Pi[f](x)(l(x)-c)\n",
    "\\end{align}\n",
    "\n",
    "** Having $g(x)$, the error in $v(x)$ no longer affect $k$**\n",
    "\n",
    "<h3>Policy gradient procedure for LMDP</h3><ol>\n",
    "    <li> Fit $\\tilde{\\\\v}(x)$ to $v(x)$ from samples via least square</li>\n",
    "    <li> Use $\\tilde{\\\\v}(x)$ to calculate average cost c</li>\n",
    "    <li> Calculate $A, k$ with $\\tilde{\\\\v}(x)$ and $c$</li>\n",
    "    <li> Fit $\\hat{\\\\v}(x)=r^Tf(x)$ from $Ar=k$</li>\n",
    "    <li> Calculate policy gradient $\\nabla_w c=\\sum_{x,x'}\\mu(x, x')(\\Pi[f](x)-f(x'))(\\hat{\\\\v}(x')-w^T f(x'))=\\sum_{x,x'}\\mu(x, x')(f(x')-\\Pi[f](x))f(x')^T(w-r)$</li>\n",
    "    </ol>\n",
    "\n",
    "<h3>Natural policy gradient</h3>\n",
    "\n",
    "Introduce nature metric $G(w)$ for the gradient of the objective function.\n",
    "$$G(w)\\triangleq\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T$$\n",
    "Then replace $\\nabla_w c$ with the natural policy gradient\n",
    "$$G(w)^{-1}\\nabla_w c=w-r$$\n",
    "\n",
    "> <font color='green'>\n",
    "**Proof Sketch**\n",
    "\\begin{align}\n",
    "\\nabla_w\\pi(x'|x)&= \\pi(x'|x)[\\sum_y \\pi(y|x)f(y)-f(x')]\\\\\n",
    "\\nabla_w \\log{\\pi(x'|x)}&= \\frac{\\nabla_w\\pi(x'|x)}{\\pi(x'|x)}\\\\\n",
    "&= \\sum_y \\pi(y|x)f(y)-f(x')\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "G(w)^{-1}\\nabla_w c &= \\frac{\\sum_{x,x'}\\mu(x, x')(f(x')-\\Pi[f](x))f(x')^T(w-r)}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')(f(x')-\\sum_{y}\\pi(y|x)f(y))f(x')^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')(-\\nabla_w \\log{\\pi(x'|x)})f(x')^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')(-\\nabla_w \\log{\\pi(x'|x)})(\\sum_y\\pi(y|x)f(y)-\\nabla_w \\log{\\pi(x'|x)})^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')\\nabla_w \\log{\\pi(x'|x)})\\nabla_w \\log{\\pi(x'|x)})^T -\\sum_{x,x'}\\mu(x, x')\\nabla_w \\log{\\pi(x'|x)})\\sum_y\\pi(y|x)f(y)^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')\\nabla_w \\log{\\pi(x'|x)})\\nabla_w \\log{\\pi(x'|x)})^T -\\sum_{x}\\mu(x)\\sum_{x'}\\pi(x'|x)\\nabla_w \\log{\\pi(x'|x)})\\sum_y\\pi(y|x)f(y)^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')\\nabla_w \\log{\\pi(x'|x)})\\nabla_w \\log{\\pi(x'|x)})^T -\\sum_{x}\\mu(x)\\sum_{x'}\\nabla_w\\pi(x'|x)\\sum_y\\pi(y|x)f(y)^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= (w-r)\\frac{\\sum_{x,x'}\\mu(x, x')\\nabla_w \\log{\\pi(x'|x)})\\nabla_w \\log{\\pi(x'|x)})^T}{\\sum_{x,x'}\\mu(x,x')\\nabla_w log\\pi(x'|x)\\nabla_wlog\\pi(x'|x)^T}\\\\\n",
    "    &= w-r\n",
    "\\end{align}\n",
    "</font>\n",
    "** When gradient is 0, $w=r$.**\n",
    "\n",
    "\n",
    "<h3>Opt: Gauss-Newton Method</h3>\n",
    "\n",
    "Besides policy gradient, there are other options to solve an optimal $v^*(x)$.\n",
    "\n",
    "**Option 1: `approximate policy iteration`**:<ol>\n",
    "<li> Given the policy parameter $w^{(i)}$ at iteration i, solve an approximated feature weight $r^{(i)}$.</li>\n",
    "<li> Use approximation $w^{(i+1)}\\approx r^{(i)}$</li>\n",
    "</ol>\n",
    ">Equivalent to natural gradient method with step size 1 $$w^{(i+1)}=w^{(i)}+1\\cdot(r^{(i)}-w^{(i)})$$\n",
    "\n",
    "**Option 2: `approximate value iteration`** solves a fix-point problem by setting $v^*(x)=w^Tf(x)$. \n",
    "\n",
    "Define $e(x,w)\\triangleq w^Tf(x)-q(x)+\\log{\\sum_y p(y|x)exp(-w^Tf(y))}$\n",
    "\n",
    "$$\\nabla_w e(x, w)=f(x)-\\sum_y\\frac{p(y|x)exp(-w^Tf(y))}{\\sum_s p(s|x)exp(-w^Tf(s))}f(y)=f(x)-\\Pi[f](x)=g(x)$$\n",
    "\n",
    "Then $e(x, w+\\delta w)\\approx e(x+w)+\\delta w^T \\nabla_w e(x,w)=e(x+w)+\\delta w^T g(x)$.\n",
    "\n",
    "Adding average cost $c$, the loss function in each iteration is \n",
    "$$min_{c,\\delta w} \\sum_x\\bar{\\\\\\mu}(x)(c+e(x,w)+\\delta w^T g(x)$$\n",
    "\n",
    "where $\\bar{\\\\\\mu}(x)$ can be fixed or be the on-policy sample distribution $\\mu(x, w)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Porblems: Policy Gradient Method\n",
    "==================\n",
    "\n",
    "<h3>Policy gradient for general parameterization</h3>\n",
    "\n",
    "**`Controlled Ito diffusion`**:\n",
    "$$dx=b(x,u)dt+C(x)dw$$\n",
    "where $w(t)$ is a standard multidimensional Brownian motion process, and $u$ is a control input. \n",
    "\n",
    "**`Hamilton-Jacobi-Bellman (HJB)** equation`** for average cost $c$ and cost-to-go $v(x)$:\n",
    "$$c=l(x,\\pi(x))+L[v](x)$$\n",
    "$$L[v](x)\\triangleq b(x,\\pi(x))^T\\nabla_x v(x)+\\frac{trace(C(x)C(x)^T\\nabla_{xx}v(x)}{2}$$\n",
    "\n",
    "Then $\\int \\mu(x)L[f](x)dx=0$\n",
    "\n",
    "> <font color='green'>\n",
    "**Proof Sketch**\n",
    "    ?????\n",
    "\n",
    "Considering policy parameterization $u=\\pi(x,w)$, then $\\nabla_w c=\\nabla_w l(x)+\\nabla_w b(x)^T \\nabla_x v(x) + L[\\nabla_w v](x)$\n",
    "> <font color='green'>\n",
    "**Proof Sketch**\n",
    "\\begin{align}\n",
    "\\nabla_w c &=\\nabla_w [l(x,\\pi(x))+L[v](x)]\\\\\n",
    "    &= \\nabla_w l(x) + \\nabla_w [b(x,\\pi(x))^T\\nabla_x v(x)+\\frac{trace(C(x)C(x)^T\\nabla_{xx}v(x)}{2}]\\\\\n",
    "    &= \\nabla_w l(x) + (\\nabla_w b(x, \\pi(x)))^T\\nabla_x v(x) + [b(x,\\pi(x))^T\\nabla_{x,w} v(x)+ \\frac{trace(C(x)C(x)^T\\nabla_{xxw}v(x)}{2}]\\\\\n",
    "    &= \\nabla_w l(x)+\\nabla_w b(x)^T \\nabla_x v(x) + L[\\nabla_w v](x)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
