{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://arxiv.org/abs/1805.07708\">\n",
    "A_Lyapunov-based Approach to Safe Reinforcement Learning</a></h1>\n",
    "by Yinlam Chow et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Highlights</h2>\n",
    "\n",
    "* Present a method for constructing Lyapunov functions in RL\n",
    "* Extend MDP to constrained MDP where constratins on the cumulative costs are added\n",
    "* Gurantee the globbal safety of a behavior policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>\n",
    "\n",
    "Constrained MDP (CMDP) extends MDP by introducing cost constraints. A policy is safe if it satisfies the cost constratins. However, CMDP is not very popular in RL of which one reason is that the model of the MDP may be unknown. \n",
    "> <font color='green'>Previously in <a href=\"https://link.springer.com/article/10.1007/s001860050035\">Constrained Markov decision processes with total cost criteria: Lagrangian approach and dual linear program</a> and <a href=\"https://arxiv.org/abs/1109.2147\">Risk-Sensitive Reinforcement Learning Applied to Control under Constraints</a>, at least in the latter one, a weighted constraint cost is directly added to the cost function, thus appearing to be a Larangian method in essense.</font> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Experiments</h2>\n",
    "\n",
    "The environments are modeled by `Markov Decision Processes (MDP)`.\n",
    "<li>State space is $S$.</li>\n",
    "<li>Action space is $A$.</li>\n",
    "<li>Transition function is $T:S\\times A\\rightarrow \\Delta S$ where $\\Delta S$ is the set of all probability distributions over $S$.</li>\n",
    "<li>Reward function is $R: S\\times A\\rightarrow \\Re$.</li> \n",
    "\n",
    "In traditional RL frameworks, the agent's goal is to maximizes the cumulative reward. However, the reward function maynot capture everything that matters. Therefore, a performance function $R^*: S\\times A\\rightarrow \\Re$ that is unobservable to the agent is additionally formalized to captures both the agent's objective and the safety of its behavior. \n",
    "\n",
    "This paper uses a grid-world environment to simplify the learning problem and limit confounding factors. An agent that fails to behave safely in grid-world environment is unlikely to be adequate to safety-critical tasks in the real world. Each cell in the grid-world can be empty or occupied by a wall or other objects. At each time step, the agent can choose to move in one of 4 directions. Several environments contain a goal cell, $G$, by moving to which the agent can get a $+50$ reward and end the episode. For any other cells, the reward is set to be $-1$ to encourage the agent to finish the episode sooner. The cumulative reward of one episode that lasts for $T$ time steps is $\\sum_{i=0}^{i=T} R(s_{t=i}, a_{t=i})$. Note that there is no discount factor.\n",
    "\n",
    "<h3>3.1 Safe interruptibility </h3>\n",
    "<img src=\"fig1.jpg\" alt=\"Alt text\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
