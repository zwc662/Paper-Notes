{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://arxiv.org/abs/1606.03476\">\n",
    "Generative Adversarial Imitation Learning</a></h1>\n",
    "Jonathan Ho, Stefano Ermon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary</h2>\n",
    "\n",
    "* Reformulated IRL as a dual of an `occupancy measure` matching problem\n",
    "\n",
    "* Drew an analogy between IRL and GAN\n",
    "\n",
    "* Proposed a model-free imitation learning algorithm utilizing the connection between IRL and GAN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Motivation</h2>\n",
    "\n",
    "* Many IRL algorithms are expensive to run, requiring solving a reinforcement learning problem in an inner loop.\n",
    "\n",
    "* IRL learns a cost function but does not tell learner how to act\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A General Framework for IRL</h2>\n",
    "\n",
    "<h3>Basis</h3>\n",
    "\n",
    "* State space is $S$; \n",
    "* Action space is $A$;\n",
    "* Dynamics model $P(s'|s,a)$ although not used; \n",
    "* Cost $c:S\\times A\\rightarrow \\mathbb{R}$\n",
    "\n",
    "<h3>RL problem</h3>\n",
    "\n",
    "Having a cost function $c$ already learnt, RL problem aims at finding the corresponding policy that has high entropy and low cost.\n",
    "$$RL(c)=argmin_{\\pi\\in\\Pi}-H(\\pi)+\\mathbb{E}_\\pi[c(s,a)]$$\n",
    "where $H(\\pi)\\triangleq -\\sum_{(s,a)} \\rho_\\pi(s,a)log\\pi(a|s) $ is defined as $\\gamma$-discounted causal entropy and $\\rho_\\pi(s,a)=\\pi(a|s)\\sum^\\infty_{t=0} \\gamma^t P(s_t=s|\\pi)$ is the occupancy measure for a policy.\n",
    "><font color=\"green\">The $\\gamma$-discounted causal entropy of a policy $\\pi$ is \n",
    "\\begin{eqnarray}\n",
    "H(\\pi)&=&-\\sum_{(s,a)}\\rho_\\pi(s,a)log\\pi(a|s)\\\\\n",
    "&=&-\\sum_{(s,a)}\\rho_\\pi(s)\\pi(a|s)log\\pi(a|s)\\\\\n",
    "&=&-\\sum_{s}\\rho_\\pi(s)\\sum_{a}\\pi(a|s)log\\pi(a|s)\\\\\n",
    "&=&\\sum_{s}\\rho_\\pi(s)H(\\pi(\\cdot|s))    \n",
    "\\end{eqnarray}\n",
    "This entropy is not the mean of the entropy $H(\\pi(\\cdot|s))$ of each state $s$, because\n",
    "\\begin{eqnarray}\n",
    "\\sum_s\\rho_\\pi(s)&=&\\sum_s\\sum^\\infty_{t=0}\\gamma^t Prob(s_t=s|\\pi)\\\\\n",
    "&=&\\sum^\\infty_{t=0}\\gamma^t \\sum_s prob(s_t=s|\\pi)\\\\\n",
    "&=&\\sum^\\infty_{t=0}\\gamma^t\\\\\n",
    "&=&\\frac{1}{1-\\gamma}\\neq 1\n",
    "\\end{eqnarray}\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>IRL problem</h3>\n",
    "\n",
    "Assume that the set of possible cost function is $C$, e.g. $C=\\mathbb{R}^{S\\times A}$, then an IRL formula with entropy regularizer is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "IRL({\\pi_E})&=&argmax_{c\\in C} (min_{\\pi\\in\\Pi}(-H(\\pi)+\\mathbb{E}_\\pi[c(s,a)]))-\\mathbb{E}_{\\pi_E}[c(s,a)] \\\\\n",
    "&=&argmax_{c\\in C} min_{\\pi\\in\\Pi}(-H(\\pi)+\\mathbb{E}_\\pi[c(s,a)]-\\mathbb{E}_{\\pi_E}[c(s,a)]) \n",
    "\\end{eqnarray}\n",
    "It can be proved that the cost of policy $\\pi$ under cost function $c$ can be evaluated by using the occupancy measure $\\rho_\\pi(s,a)$ such that $\\mathbb{E}_\\pi[c(s,a)]=\\sum_{s,a}\\rho_\\pi(s,a)c(s,a)=\\mathbb{E}_{\\rho_\\pi}[c(s,a)]$. Likewise, the entropy term $H(\\pi)$ can also be represented as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(\\pi)&=&-\\sum_{(s,a)}\\rho_\\pi(s,a)log\\pi(a|s)\\\\\n",
    "&=&-\\sum_{(s,a)}\\rho_\\pi(s,a)log\\frac{\\rho_\\pi(s,a)}{\\sum_{a'}\\rho_\\pi(s,a')}\\\\\n",
    "&=&\\bar{\\\\H}(\\rho_\\pi)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Use occupancy measure $\\rho_\\pi$ to replace $\\pi$ in $IRL({\\pi_E})$ and get $IRL(\\rho_{\\pi_E})$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "IRL(\\rho_{\\pi_E})&=&argmax_{c\\in C} min_{\\rho_\\pi\\in D}(-\\bar{\\\\H}(\\rho_\\pi)+\\sum_{s,a}\\rho_\\pi(s,a)c(s,a)-\\sum_{s,a}\\rho_{\\pi_E}(s,a)c(s,a))\\\\\n",
    "&=&argmax_{c\\in C} min_{\\rho_\\pi\\in D}[-\\bar{\\\\H}(\\rho_\\pi)+\\sum_{s,a}(\\rho_\\pi(s,a)-\\rho_{\\pi_E}(s,a))c(s,a)]\n",
    "\\end{eqnarray}\n",
    "where $D$ is the feasible set for $\\rho$ determined by the set of policies. The concavity and convexity of the components in the formula make sure that the min\\max problem has a saddle point.\n",
    "\n",
    "Obviously, $IRL(\\rho_{\\pi_E})$ is the dual problem of the primal\n",
    "$$min_{\\rho_\\pi\\in D} -\\bar{\\\\H}(\\pi_\\pi)\\qquad s.t. \\rho_\\pi(s,a)=\\rho_{\\pi_E}(s,a)\\ \\forall (s,a)\\in S\\times A$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>General IRL formulation</h3>\n",
    "\n",
    "**According to the paper, to avoid overfitting in the finite dataset (to be clarified)**, a convex cost function regularizer $\\psi:\\mathbb{R}^{S\\times A}\\rightarrow \\bar{\\\\\\mathbb{R}}$ can be introduced to IRL formula such that\n",
    "\n",
    "$$IRL(\\rho_{\\pi_E})=argmax_{c\\in C} min_{\\pi\\in\\Pi}(-\\psi(c)-H(\\pi)+\\mathbb{E}_\\pi[c(s,a)]-\\mathbb{E}_{\\pi_E}[c(s,a)])$$\n",
    "\n",
    "The IRL formula can be generalized to other existing IRL algorithms by accustomzing the regularizer $\\psi(c)$. \n",
    "\n",
    "\n",
    "<em>For instance, the apprenticeship learning formula is\n",
    "\n",
    "$$AL(\\rho_{\\pi_E})=argmax_{c\\in C} \\mathbb{E}_\\pi[c(s,a)] -\\mathbb{E}_{\\pi_E}[c(s,a)]$$\n",
    "where $c(s,a)={\\bf \\omega}^T {\\bf f(s,a)}$ is a linear combination of a set of feature functions ${\\bf f}(s,a)=[f_1(s,a), f_2(s,a),\\ldots, f_k(s,a)]^T$ with $||{\\bf \\omega}||_2\\leq 1$. \n",
    "\n",
    "Then $IRL(\\rho_{\\pi_E})$ recovers $AL(\\rho_{\\pi_E})$ by defining \n",
    "$$\n",
    "\\psi(c) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad if\\ c\\in C \\\\\n",
    "            \\infty &\\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "</em>\n",
    "\n",
    "Define the convex conjugate of $\\psi(c)$ as $\\psi^*(x)=sup_c x^T c-\\psi(c)$. Let $x=\\rho_\\pi-\\rho_{\\pi_E}$, then\n",
    "\\begin{eqnarray}\n",
    "IRL(\\rho_{\\pi_E})&=&argmax_{c\\in C} min_{\\pi\\in\\Pi}[-\\psi(c)-\\bar{\\\\H}(\\rho_\\pi)+\\sum_{s,a}(\\rho_\\pi(s,a)-\\rho_{\\pi_E}(s,a))c(s,a)]\\\\\n",
    "&=& \\psi^*(c)-\\bar{\\\\H}(\\rho_\\pi)\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generative Adversarial Imitation Learning</h2>\n",
    "\n",
    "* Define the regularizer for GAIL to as follows\n",
    "$$\\psi_{GA}(c)\\triangleq\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\mathbb{E}_{\\rho_{\\pi_E}}[g(c(s,a))] & \\quad c<0 \\\\\n",
    "           +\\infty & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.$$\n",
    "where \n",
    "$$g(x)=\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            -x-log(1-e^x) & \\quad x<0 \\\\\n",
    "           +\\infty & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.$$\n",
    "    \n",
    "> <font color=\"green\">This regularizer is convex and is positively correlated to $c$ when $c$ approximiates $0$ and becomes $-\\infty$ when $c\\geq 0$. For a state-action pair $(s,a)$ with $c(s,a)<0$, when $\\rho_{\\pi_E}(s,a)$ is high, $\\rho_{\\pi_E}(s,a)c(s,a)$ is highly negative. Therefore the regularizer $\\psi_{GA}(c)$ lays a heavy penalty on $c(s,a)>0$ when $\\rho_{\\pi_E}(s,a)>0$. </font>\n",
    "\n",
    "Then conjugate of $\\psi_{GA}(c)$ is\n",
    "$$\\psi^*_{GA}(\\rho_\\pi-\\rho_{\\pi_E})=sup_{c\\in C}\\sum_{(s,a)}(\\rho_\\pi(s,a)-\\rho_{\\pi_E}(s,a))c(s,a)-\\psi_{GA}(c)$$\n",
    "\n",
    "Rearrange the items in $\\psi^*_{GA}(\\rho_\\pi-\\rho_{\\pi_E})$ such that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\psi^*_{GA}(\\rho_\\pi-\\rho_{\\pi_E})&=&sup_{c\\in C}\\sum_{(s,a)}(\\rho_\\pi(s,a)-\\rho_{\\pi_E}(s,a))c(s,a)-\\psi_{GA}(c)\\\\\n",
    "&=&sup_{c\\in C} \\sum_{(s,a)}\\rho_\\pi(s,a)c(s,a)-\\sum_{(s,a)}\\rho_{\\pi_E}(s,a)c(s,a)-\\sum_{(s,a)}\\rho_{\\pi_E}[g(c(s,a))]\\\\\n",
    "&=&sup_{c\\in C} \\sum_{(s,a)}\\rho_\\pi(s,a))c(s,a)+\\sum_{(s,a)}\\rho_{\\pi_E}(s,a)[-c(s,a)-g(c(s,a)]\\\\\n",
    "&=&sup_{c\\in C} \\sum_{(s,a)}\\rho_\\pi(s,a))c(s,a)+\\sum_{(s,a)}\\rho_{\\pi_E}(s,a)[-c(s,a)+c(s,a)+log(1-e^{c(s,a)})]\\\\\n",
    "&=&sup_{c\\in C} \\sum_{(s,a)}\\rho_\\pi(s,a))c(s,a)+\\sum_{(s,a)}\\rho_{\\pi_E}(s,a)[log(1-e^{c(s,a)})]\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Let $c(s,a)=log\\frac{1}{1+e^{-\\gamma(s,a)}}$ where $\\gamma(s,a)\\in(-\\infty, +\\infty)$ can be the output of an NN model given input $(s,a)$ while $c(s,a)$ is logistic loss. Then $log(1-e^{c(s,a)})=log(1-\\frac{1}{1+e^{-\\gamma(s,a)}})$.\n",
    "\n",
    "Rewrite the logistic loss $c(s,a)=log\\frac{1}{1+e^{-\\gamma(s,a)}}$ as $log D(s,a)$ and $log(1-e^{c(s,a)})$ as $log(1-D(s,a))$. Obviously $D(s,a)=\\frac{1}{1+e^{-\\gamma(s,a)}}\\in[0,1]$. Use $D(s,a)$ instead of $c(s,a)$ in IRL formula.\n",
    "\n",
    "$$IRL(\\rho_{\\pi_E})=min_{\\rho_\\pi}max_{D\\in(0,1)^{S\\times A}} -\\bar{\\\\H}(\\rho_\\pi)+\\mathbb{E}_{\\rho_\\pi} [log(D(s,a))]+\\mathbb{E}_{\\rho_{\\pi_E}} [log(1-D(s,a))]$$\n",
    "\n",
    "This formula resembles GAN formula if regarding $D(s,a)$ as the output of a discriminant model while $\\rho_\\pi$ as the ouput of a generative model.\n",
    "\n",
    "In addition, when optimizing w.r.t $\\rho_\\pi$, prameterize $\\pi$ with $\\theta$ such that the gradient of the loss becomes\n",
    "$$\\nabla_\\theta -\\bar{\\\\H}(\\rho_\\pi)+\\sum_{(s,a)} \\rho_{\\pi_\\theta} c(s,a)=-\\nabla_\\theta\\bar{\\\\H}(\\rho_{\\pi_\\theta})+\\mathbb{E}_{(s,a)}\\nabla_\\theta log\\pi_\\theta(a|s) Q(s,a)$$\n",
    "which recovers the policy gradient formula. Note that by solving the policy gradient problem iteratively, $\\mathbb{E}_{(s,a)}$ and $Q(s,a)$ can be obtained from last iteration. The paper chooses TRPO to finish this step.\n",
    "\n",
    "<h3>Algorithm</h3>\n",
    "\n",
    "1. Given a set of expert trajectories $\\tau_E\\sim\\pi_E$. \n",
    "\n",
    "2. Initialize a parameterized policy $\\pi_{\\theta_0}$ and a parameterized discriminator $D_{\\omega_0}$\n",
    "\n",
    "3. In iteration $i\\in\\{0,1,2,\\ldots\\}$\n",
    "    * Sample trajectories $\\tau_i$ from policy $\\pi_{\\theta_i}$\n",
    "    * Update $\\omega$ with gradient \n",
    "    $$\\mathbb{E}_{\\tau_i} [\\nabla_\\omega log(D_\\omega (s,a))] + \\mathbb{E}_{\\tau_E}[\\nabla_\\omega log(1-D_\\omega(s,a))]$$\n",
    "    * Update $\\theta$ by using TRPO with cost function $c(s,a)=log(D_{\\omega_{i+1}}(s,a))$. The gradient of the penalized loss is \n",
    "    $$\\mathbb{E}_{\\tau_i}[\\nabla_\\theta log \\pi_\\theta(a|s)Q(s,a)]-\\lambda\\nabla_\\theta H(\\pi_\\theta)$$ where $Q(s,a)=\\mathbb{E}_{\\tau_i}[log(D_{\\omega_{i+1}}(s,a))|s_0=s, a_0= a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
