{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://ieeexplore.ieee.org/abstract/document/5940562/\">\n",
    "Towards Evaluating the Robustness of Neural Networks</a></h1>\n",
    "by Nicholas Carlini et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Summary</h2>\n",
    "\n",
    "New attacks are developed to generate adversarial examples successfully on distilled NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Background</h2>\n",
    "\n",
    "* ``Neural Network`` with softmax cllasifier\n",
    "    * $F(x)=softmax\\circ F_n\\circ F_{n-1}\\circ\\ldots\\circ F_1$ where $softmax(Z(x))_i=\\frac{e^{Z(x)_i}}{\\sum e^{Z(x)_j}}$\n",
    "    * Final output is $C(x)=argmax_i F(x)_i$ and let $C^*(x)$ be the correct label of x\n",
    "\n",
    "* ``Adversarial examples`` not only exist in image classification, but also in speech recognition, malware cllasification.\n",
    "    *  **Taregeted** adversarial examples: given a class t and an input x that is not classified as t, find a new input x′ that is similar to x but classified as t.\n",
    "    *  **Untaregeted** adversarial examples: given an input x, find a new input x′ that is similar to x but classfied differently.\n",
    "\n",
    "* ``Evaluate the robustness of a NN`` in two ways\n",
    "<ul>\n",
    "    <li>attempt to prove a lower bound (*sound but difficult practically*)</li>\n",
    "    <li>construct attacks that demonstrate an upper bound (*attacks must be strong enough to support the upperbound*)</li>\n",
    "</ul>\n",
    "\n",
    "* ``Adversary attacker`` has complete access to a neural network. \n",
    ">Prior work shows that it is possible to train a substitute model given a black-box access to a target model and transfer the attacks from the substitute model to the target model.\n",
    "\n",
    "    * **Targeted adversarial attack** \n",
    "        * *Average case*: select incorrect class uniformly\n",
    "        * *Best case*: attack against all incorrect class and report the least difficult one \n",
    "        * *Worst case*: attack against all incorrect class and report the hardest one\n",
    "\n",
    "* ``Distance metrics`` measures the distortion made to the input. In this paper, various $L_p$ norms are used because of its approximations of human perceptual distance.\n",
    "> $L_p$ can be written as $||x-x'||_p=(\\sum^n_{i=1}|x_i-x'_i|^p)^{\\frac{1}{p}}$. Most of the existing work pick one of the distance metrics below.\n",
    "\n",
    "    * $L_0$ measures the  number of coordinates i such that $x_i\\neq x'_i$\n",
    "    * $L_2$ measures the standard Euclidean distance between x and x'\n",
    "    * $L_\\inf$ measures the maximum change to any of the coordinate\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1511.04508\">``Defensive distillation``</a>\n",
    "    The idea is tha adversarial hids in blind spot raised by overfitting. Using data from softer network avoid over-fitting. The idea is likely to be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Attack Algorithms</h2>\n",
    "\n",
    "<h3><a href=\"https://openreview.net/forum?id=kklr_MTHMRQjG\">L-BFGS</a></h3>\n",
    "\n",
    "Given an image x, find a different image x' that is similar to x yet is labeled differently by classifier.\n",
    "\n",
    "\\begin{equation}\n",
    "min\\qquad ||x-x'||^2_2+loss_{F,l}(x')\\\\\n",
    "s.t.\\qquad x'\\in[0, 1]^n\n",
    "\\end{equation}\n",
    "\n",
    "where $loss_{F,l}$ maps an image input to a positive value, e.g. the cross entropy function. \n",
    "\n",
    "With one constant $c>0$, the objective problem can be solved via line search and the solution is one adversarial example of minimum distance. \n",
    "\n",
    "By chaning $c$ adaptively, different or `better` adversarial example can be found.\n",
    "\n",
    "\n",
    "<h3><a href=\"https://arxiv.org/abs/1412.6572\">Fast Gradient Sign</a></h3>\n",
    "\n",
    "* Use $L_\\infty$ norm rather than $L_2$ norm.\n",
    "* Meant to be fast rather than producing minimal pertubation\n",
    "\n",
    "$$x'=x-\\epsilon\\cdot sign(\\nabla loss_{F,t}(x))$$\n",
    "\n",
    "where $\\epsilon$ is chosen to be sufficiently small so as to be undetectable. \n",
    "\n",
    "The gradient of the loss function determine which direction the pixel's intensity should be changed to minimize the loss function.\n",
    "\n",
    "><h4><a href=\"https://arxiv.org/abs/1607.02533\">Iterative Gradient Sign</a>\n",
    "takes multiple small step length $\\alpha$ than $\\epsilon$ and uses a `clip` function.\n",
    "$$x'_0=0$$\n",
    "$$x'_i=x'_{i-1}-clip_\\epsilon(\\alpha\\cdot sign (\\nabla loss_{F,t}(x'_{i-1})))$$\n",
    "$$clip_\\epsilon(\\Delta) = min\\{􏰆255, x'_{i-1}+\\epsilon, max\\{􏰂0, x'_{i-1}-\\epsilon, \\Delta􏰃􏰇\\}\\}$$\n",
    "\n",
    "<h3><a href=\"https://arxiv.org/abs/1511.07528\">JSMA</a></h3>\n",
    "\n",
    "* Greedy algorithm that picks pixels to modify one at a time and iteratively increase the confidence of target classification. \n",
    "\n",
    "* Use $\\nabla z(x)_l$ to measure the impact of each pixel on the likelihood of classifing the image as the target class $l$.\n",
    "\n",
    "* Pick the pixel that has the largest impact and modify it to increase the likelihood of class $l$. \n",
    " \n",
    "* Repeat until either false classification is achieved or the modification becomes detectable because a set threshold of pixels are modified.\n",
    "\n",
    "$$\\alpha_{p,q}=\\sum_{i\\in{p,q}}\\frac{\\partial z(x)_t}{\\partial x_i}$$\n",
    "$$\\beta_{p,q}=(\\sum_{i\\in{[p,q}}\\sum_j\\frac{\\partial z(x)_j}{\\partial x_i}-\\alpha_{p,q}$$\n",
    "\n",
    "where $\\alpha_{p,q}$ measures how much changing both pixelx $p$ and $q$ will change the target classification $t$; $\\beta_{p,q}$ measures how much changing $p$ and $q$ will change all other classification $j!=t$. The algorithm picks \n",
    "\n",
    "$$(p^*,q^*)=argmax_{(p,q)}(-\\alpha_{p,q}\\cdot\\beta_{p,q})\\cdot(\\alpha_{p,q}>0)\\cdot(\\beta_{p,q}<0)$$\n",
    "\n",
    "so that $\\alpha_{p,q}>0$ and $\\beta_{p,q}<0$, i.e. the target class becomes more likely while others become less likely. \n",
    "\n",
    "* If $z(x)$ is used, then it is called JSMA-Z attack; otherwise if $F(x)$ is used, then it is called JSMA-F attack.\n",
    "\n",
    "* For RGB image, for each pixel there are 3 color channels and $L_0=1$ means that one color of one pixel is changed.\n",
    "\n",
    "<h3><a href=\"https://arxiv.org/abs/1511.04599\">Deepfool</a></h3>\n",
    "\n",
    "* An untargeted attack technique optimized for the $L_2$ metric. \n",
    "\n",
    "* Produces closer adversarial examples than L-BFGS\n",
    "\n",
    "* Imagine the NN is totoally linear\n",
    "\n",
    "* Try to derive an optimal solution for the adversarial example iteratively until a true adversarial example is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experiment Preparation</h2>\n",
    "\n",
    "* Train two networks for the <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a> AND <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a> tasks with <a href=\"https://arxiv.org/abs/1511.04508\">defensively ditillation</a> method.\n",
    "\n",
    "* Achieved $99.5\\%$ accuracy on MNIST AND $80\\%$ ON CIFAR-10.\n",
    "\n",
    "* Use a pre-trained <a href=\"https://arxiv.org/abs/1512.00567\">Inception v3 network</a> for <a href=\"http://www.image-net.org\">ImageNet</a>.\n",
    "\n",
    "* Achieved $96\\%$ top-5 accurary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Construct Adversarial Examples</h2>\n",
    "\n",
    "<h3>Optimization problem formulation</h3>\n",
    "\n",
    "Let $\\delta$ be the change made to the original input $x$, $t$ the target class. The initial formulation of adversarial example is the solution of \n",
    "\n",
    "\\begin{equation}\n",
    "min_\\delta\\qquad ||x,x+\\delta||_p\\\\\n",
    "s.t.\\qquad C(x+\\delta)=t\\\\\n",
    "    \\qquad x+\\delta\\in[0, 1]^n\n",
    "\\end{equation} \n",
    "\n",
    "As $C(x+\\delta)=t$ is highly non-linear, define an objective function $f(x)$ such that $f(x+\\delta)\\leq0\\ iff\\ C(x+\\delta)=t$. Then the optimization problem becomes\n",
    "\n",
    "\\begin{equation}\n",
    "min_\\delta\\qquad ||x,x+\\delta||_p\\\\\n",
    "s.t.\\qquad f(x+\\delta)\\leq 0\\\\\n",
    "    \\qquad x+\\delta\\in[0, 1]^n\n",
    "\\end{equation}\n",
    "\n",
    "**In practice, to simplify the optimization problem, use a surrogate objective function with a constant c>0.**\n",
    "\n",
    "\\begin{equation}\n",
    "min_\\delta\\qquad ||x,x+\\delta||_p+c\\cdot f(x+\\delta)\\\\\n",
    "s.t.\\qquad x+\\delta\\in[0, 1]^n\n",
    "\\\\\\qquad c>0\n",
    "\\\\where\\ f(x)=max(max_{i\\neq t}(z(x+\\delta)_i)-z(x+\\delta)), 0)\\ or\\ other\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"./fig1.png\"><center>The larger c is, the larger the distance of perturbation becomes; when c is small, the attack success rate is low; when c increases, the attach success rate grows rapidly to 100%. In implementation, a modified binary search is used to choose c</center></img> \n",
    "\n",
    "For the constraint $x+\\delta\\in[0, 1]^n$, three approaches are used in combination with <a href=\"https://arxiv.org/abs/1412.6980\">Adam optimizer</a>:\n",
    "* ``Projected gradient descent`` performs gradient descent then clips the updates to be within the constraint. ``cons``: Work poorly for gradient descent approaches that have a complicated update step.\n",
    "    \n",
    "* ``Clipped gradient descent`` performs gradient descent then clips the updates in the objective function $f(x+\\delta)$ to be $f(min(max(x+\\delta, 0), 1)$. ``cons``: x can get stuck at a value that is too large\n",
    "    \n",
    "* ``Change of variables`` use $\\delta=\\frac{tanh(w_i)+1}{2}-x$ and optimize w.r.t $w$ instead of $\\delta$, since $\\delta+x=\\frac{tanh(w_i)+1}{2}\\in[0,1]$.\n",
    "\n",
    "<h3>Evaluation and discussion</h3> \n",
    "\n",
    "The choose of loos function $f(x)$ impact the result most significantly. \n",
    "\n",
    "* It turns out that $Z(x)_t$ is mostly linear w.r.t $\\delta$. Then $F(x)_t$ becomes a logistic. \n",
    "\n",
    "* $\\nabla f(x)$ is small at initial x. To make $||x+\\delta-x||_p <c(f(x+\\delta)-f(x))$ with small $\\delta$, $c$ has to be large.\n",
    "\n",
    "* But $\\nabla f(x+\\delta)$ increases exponentially with $\\delta$, it makes constant $c$ overly large.\n",
    "\n",
    "If the discrete color intensity is rounded when gradient is calculuated, then the quality of the adversarial example will be degraded, especially when the change to the initial image is small. Discretization and greedy search are implemented to improve the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attacks with three different distance metrics</h2>\n",
    "\n",
    "<h3>$L_2$ attack</h3>\n",
    "\n",
    "The optimization function is\n",
    "\n",
    "\\begin{equation}\n",
    "min_w ||\\frac{tanh(w_i)+1}{2}-x||^2_2 + c\\cdot f(\\frac{tanh(w_i)+1}{2})\n",
    "\\\\where\\ f(x+\\delta)=max(max\\ {Z(x+\\delta)_i:i!=t}-Z(x+\\delta)_t, -\\kappa\n",
    "\\end{equation}\n",
    "\n",
    "Adjust $\\kappa$ to control the confidence of misclassification. The figure below shows the adversarial examples generated via MNIST.\n",
    "\n",
    "<img src=\"./fig2.png\"><center>Almost all attacks are visually indistinguishable.</center></img>\n",
    "\n",
    "To avoid stucking in local optimum, multiple random starting points are chosen within $r$-balls around original pixels.\n",
    "\n",
    "<h3>$L_0$ attack</h3>\n",
    "\n",
    "$L_0$ distance is non-differentiable. An iterative algorithm is used. \n",
    "\n",
    "* In each iteration, use $L_2$ attack to find pixels that have low effect $argmin_x \\delta \\nabla f(x+\\delta)$ on the classifier output. \n",
    "* Fix the values of the pixels found in all previous iterations and keep constant in the iterations afterwards. \n",
    "* Finally $L_2 attack$ cannot find an adversarial example. A possibly minimal set of pixels that have significant effect on the classifier output is left.\n",
    "\n",
    "Each time when soving $L_2$ attack, a small $c$ is used initially. If attack fails, $c$ will be doubled and attack is restarted until either attack succeeds or $c$ reaches a threshold.\n",
    "\n",
    "<img src=\"./fig3.png\"><center>Almost all attacks are visually indistinguishable.</center></img>\n",
    "\n",
    "In the figure above, the attacks on MNIST are visually noticeable.\n",
    "\n",
    "<h3>$L_\\infty$ attack</h3>\n",
    "\n",
    "The optimization problem is\n",
    "\n",
    "$$ min_\\delta c\\cdot f(x+\\delta)+||\\delta||_\\infty$$\n",
    "\n",
    "Gradient descent performs poorly. An iterative attack is used with a penalty appproximating the $L_\\infty$ norm.\n",
    "\n",
    "$$ min_\\delta c\\cdot f(x+\\delta)+\\sum_i[max(\\delta_i-\\tau, 0)]$$\n",
    "\n",
    "In each iteration, $c$ is initially a small value. If attack fails, $c$ is doubled. Search $c$ until attack succeeds or $c$ exceeds a threshold.\n",
    "\n",
    "After each iteration, if $\\delta_i<\\tau$ for all $i$, $\\tau$ is reduced; otherwise, the iteration terminates.\n",
    "\n",
    "<img src=\"./fig4.png\"><center>Almost all attacks are visually indistinguishable.</center></img>\n",
    "\n",
    "In all $L_0, L_2, L_\\infty$ attacks, when $7$ is classifed as $6$, the attacks are distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attack Evaluation in Comparison with Prior Work</h2>\n",
    "\n",
    "* Attacks find closer adversarial examples than the previous state-of-the-art attacks, and never fail to find an adversarial example while others occasionally fail\n",
    " \n",
    "* As the learning task becomes increasingly more difficult, the previous attacks produce worse results, due to the complexity of the model. In contrast, our attacks perform even better as the task complexity increases.\n",
    " \n",
    "* **It is important to realize that the results between models are not directly comparable.**\n",
    "\n",
    "<img src=\"./fig5.png\"><center>When the initial image is all black and while, the perturbation that the attacks make for the target misclassification.</center></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Attacking Defensive Distillation</h2>\n",
    "\n",
    "<h3>Distillation</h3>\n",
    "    * First train a teacher network on a standard training set. \n",
    "    * Use the teacher network to re-label the training set with its output. \n",
    "    * Train a smaller distilled network with the re-labeled training set.\n",
    "    >Re-label means that for each input use labels with probabilities instead of one single ground true label.\n",
    "    \n",
    "<h3>Defensive distillation</h3>\n",
    "    * The distilled network is no longer smaller than the teacher network\n",
    "    * Use a large `distillation temperature $T$` to force the distilled model to become more confident in its predictions.$$sofmax(x, T)_i=\\frac{e^{x_i/T}}{\\sum_j e^{x_j/T}}=softmax(x/T, 1)$$\n",
    "    * Training steps:\n",
    "<ol>\n",
    "    <li>Build a teacher network with temperature $T$.</li>\n",
    "    <li>Train the teacher network with a standard traininig set.</li>\n",
    "    <li>Use the trained teacher network to re-label the training set.</li>\n",
    "    <li>Train the distilled network with temperature $T$ on the re-labeled training set.</li>\n",
    "    <li>When testing the distilled network, use temperature 1.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Existing attacks fails</h3>\n",
    "\n",
    "* `L-BFGS`, `JSMA-F`, `Fast Gradient Sign` and `Deepfool` attacks with standard objective function fail because $\\nabla F()\\approx 0$ almost everywhere.\n",
    "    >Temperature $T\\rightarrow 1$ is equivalent to $x\\rightarrow T\\cdot x$. The output of softmax with correct class becomes far more close to 1 than that of other classes. And the small confidence of other class is eliminated due to arithmetic rounding.\n",
    "    \n",
    "* JSMA-Z fails. \n",
    "    * It regard the changes in $Z()$ of all classes as equally important\n",
    "    \n",
    "    > If the changing a pixel will lead to similar changes in the $Z()$ value of the least likely and the most likely classes, then this pixels will not be changed. \n",
    "    \n",
    "    * Temperature changes magnifies the changes of all classes. Hence the attack refuse to make any change.\n",
    "    \n",
    "<h3>Defensive Distillation is still vulnerable</h3>\n",
    "\n",
    "* The attack introduced in this paper succeeds with $100%$ success probability for all three distance metrics.\n",
    "\n",
    "* Temperature variabtion has no effect on defending $L_2$ attack.\n",
    "\n",
    "* Transferable attack can be learnt via standard model. \n",
    "    * Use $L_2$ attack to find ``high confident`` adversarial examples in a standard network\n",
    "    * Adversarial examples of standard network successfully attack the defensively distilled network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "* Defensive distillation is not as robust as it has been claimed to be.\n",
    "\n",
    "* The attack introduced in this paper is powerful and can be a new robustness metric.\n",
    "\n",
    "* Transferable attack should be a part of the robustness test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
