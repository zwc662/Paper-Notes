{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"https://arxiv.org/abs/1412.6572\">\n",
    "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></h1>\n",
    "Chelsea Finn et al.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary</h2>\n",
    "\n",
    "* **`Model Agnostic Meta-Learning (MAML)`**\n",
    "    * is **compatible** with any model trained with gradient descent.\n",
    "\n",
    "    * is applicable to **variety** of learning tasks, including classification, regression and reinforcement learning.\n",
    "\n",
    "    * trains model with a **small number** of gradient steps as well as a small amount of training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Theory of MAML</h2>\n",
    "\n",
    "A learning task $\\mathcal{T}=\\{\\mathcal{L}(x_1, a_1,\\ldots, x_H, a_H), q(x_1), q(x_{t+1}|x_t, a_t), H\\}$ drawn from distribution $p(\\mathcal{T})$ consists\n",
    "* a loss function $\\mathcal{L}$,\n",
    "* a distribution over initial observations $q(x_1)$,\n",
    "* a transition distribution $q(x_{t+1}|x_t, a_t)$,\n",
    "* an episode length $H$\n",
    "\n",
    "Assume that a model is represented by a parameterized function $f_\\theta$. When trained on single task $\\mathcal{T}_i$, the gradient update is \n",
    "$$\\hat{\\\\\\theta}'_i= \\hat{\\\\\\theta}-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_i} (f_\\theta)$$\n",
    "\n",
    "Consider the task distribution then the meta-objective is\n",
    "$$min_\\theta \\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T})}\\mathcal{L}_{\\mathcal{T}_i} (f_{\\hat{\\\\\\theta}'_i})\n",
    "\\Rightarrow min_\\theta \\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T}_i)} \\mathcal{L}_{\\mathcal{T}_i}(f_{\\hat{\\\\\\theta}-\\alpha\\nabla_\\theta\\mathcal{L}_{\\mathcal{T}_i}(f_\\theta)})$$\n",
    "\n",
    "When training on multiple tasks, use stochastic gradient descent (SGD) to update $\\theta$\n",
    "$$\\hat{\\\\\\theta}'\\leftarrow \\hat{\\\\\\theta}-\\beta\\nabla_\\theta \\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T}_i)} \\mathcal{L}_{\\mathcal{T}_i}(f_{\\hat{\\\\\\theta}'_i})$$\n",
    "\n",
    "\n",
    "**Algorithm**\n",
    "<ol>\n",
    "    <li>Initialize parameter $\\theta$</li>\n",
    "    <li>Sample tasks $\\mathcal{T}\\sim p(\\mathcal{T})$</li>\n",
    "    <li>For every sampled $\\mathcal{T}$, evaluate $\\hat{\\\\\\theta}'_i= \\hat{\\\\\\theta}-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_i} (f_\\theta)$ by using same number of data samples.</li>\n",
    "    <li>Update $\\hat{\\\\\\theta}'\\leftarrow \\hat{\\\\\\theta}-\\beta\\nabla_\\theta \\sum_{\\mathcal{T}_i\\sim p(\\mathcal{T}_i)} \\mathcal{L}_{\\mathcal{T}_i}(f_{\\hat{\\\\\\theta}'_i})$\n",
    "</ol>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MAML Instances</h2>\n",
    "\n",
    "<h3>Supervised Regression and Classification</h3>\n",
    "\n",
    "Horizon $H=1$ and consider single data points $(x^{(j)}, y^{(j)})$ with no timestep subscript from data batch.\n",
    "\n",
    "For **regression task**\n",
    "$$\\mathcal{L}_{\\mathcal{T}_i}(f_\\theta)=\\sum_{(x^{(j)}, y^{(j)})\\sim \\mathcal{T}_i} ||f_\\theta(x^{(j)}) - y^{(j)}||^2_2$$\n",
    "\n",
    "For **classification task**\n",
    "$$\\mathcal{L}_{\\mathcal{T}_i}(f_\\theta)=\\sum_{(x^{(j)}, y^{(j)})\\sim\\mathcal{T}_i}  [y^{(j)}log f_\\theta(x^{(j)}) + (1 - y^{(j)})log(1-f_\\theta(x^{(j)}))]$$\n",
    "\n",
    "To update $\\hat{\\\\\\theta}'$ for MAML at step 4, new data batches have to be sampled for each task after $\\hat{\\\\\\theta}'_i$ has been updated at step 3.\n",
    "\n",
    "<h3>Reinforcement Learning</h3>\n",
    "\n",
    "Assume that horizon of the task is $H>1$ and for each task $\\mathcal{T}_i$, the example data set $\\{(x_1,a_1), (x_2,a_2), \\ldots, (x_H, a_H)\\}$ is drawn from a distribution $q_{\\mathcal{T}_i}$. Then the loss function of task $\\mathcal{T}_i$ is $$\\mathcal{L}_{\\mathcal{T}_i}(f_\\theta)=-\\mathbb{E}_{(x_t, a_t)\\sim f_\\theta, q_{\\mathcal{T}_i}}[\\sum^H_{t=1} R_i(x_t, a_t)]$$\n",
    "Pratically, expectation can be simulated by the mean of a batch of trajectories. Likewise, before updating MAML parameter $\\hat{\\\\\\theta}'$ at step 4, new trajectory batches have to be sampled for each task after $\\hat{\\\\\\theta}'_i$ has been updated at step 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
